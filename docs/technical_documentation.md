# æ–‡æ¡£è½¬æ¢è°ƒåº¦ç³»ç»Ÿ - æŠ€æœ¯æ–‡æ¡£

## ğŸ“‹ ç›®å½•

1. [ç³»ç»Ÿæ¦‚è¿°](#ç³»ç»Ÿæ¦‚è¿°)
2. [æ¶æ„è®¾è®¡](#æ¶æ„è®¾è®¡)
3. [æ ¸å¿ƒç»„ä»¶](#æ ¸å¿ƒç»„ä»¶)
4. [ä»»åŠ¡å¤„ç†é€»è¾‘](#ä»»åŠ¡å¤„ç†é€»è¾‘)
5. [é˜Ÿåˆ—ç³»ç»Ÿ](#é˜Ÿåˆ—ç³»ç»Ÿ)
6. [é”™è¯¯å¤„ç†æœºåˆ¶](#é”™è¯¯å¤„ç†æœºåˆ¶)
7. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
8. [éƒ¨ç½²æŒ‡å—](#éƒ¨ç½²æŒ‡å—)
9. [æ‰©å±•å¼€å‘](#æ‰©å±•å¼€å‘)

## ğŸ¯ ç³»ç»Ÿæ¦‚è¿°

æ–‡æ¡£è½¬æ¢è°ƒåº¦ç³»ç»Ÿæ˜¯ä¸€ä¸ªåŸºäºFastAPIçš„æ™ºèƒ½æ–‡æ¡£è½¬æ¢å¹³å°ï¼Œé‡‡ç”¨å¼‚æ­¥ä»»åŠ¡è°ƒåº¦æ¶æ„ï¼Œæ”¯æŒOfficeæ–‡æ¡£ã€PDFç­‰å¤šç§æ ¼å¼çš„æ‰¹é‡è½¬æ¢å¤„ç†ã€‚

### æ ¸å¿ƒç‰¹æ€§

- **å¼‚æ­¥å¹¶å‘**: æ”¯æŒå¤šä»»åŠ¡å¹¶å‘å¤„ç†ï¼Œæœ€å¤§å¹¶å‘æ•°å¯é…ç½®
- **æ™ºèƒ½é‡è¯•**: è‡ªåŠ¨é‡è¯•å¤±è´¥ä»»åŠ¡ï¼Œæœ€å¤šé‡è¯•3æ¬¡
- **å®æ—¶ç›‘æ§**: ä»»åŠ¡çŠ¶æ€å®æ—¶è·Ÿè¸ªå’Œé˜Ÿåˆ—ç»Ÿè®¡
- **æ¨¡å—åŒ–è®¾è®¡**: æ¸…æ™°çš„ä¸‰å±‚æ¶æ„ï¼Œæ˜“äºç»´æŠ¤å’Œæ‰©å±•
- **å¤šæ ¼å¼æ”¯æŒ**: Officeæ–‡æ¡£ã€PDFæ–‡æ¡£çš„å¤šç§è½¬æ¢ç»„åˆ
- **æ‰¹é‡å¤„ç†**: æ”¯æŒç›®å½•çº§åˆ«çš„æ‰¹é‡æ–‡æ¡£è½¬æ¢

### æŠ€æœ¯æ ˆ

| ç»„ä»¶ | æŠ€æœ¯ | ç‰ˆæœ¬ | ç”¨é€” |
|------|------|------|------|
| **Webæ¡†æ¶** | FastAPI | 0.104.1 | HTTP APIæœåŠ¡ |
| **ASGIæœåŠ¡å™¨** | Uvicorn | 0.24.0 | å¼‚æ­¥WebæœåŠ¡å™¨ |
| **å¼‚æ­¥å¤„ç†** | Python asyncio | 3.11+ | å¼‚æ­¥ä»»åŠ¡è°ƒåº¦ |
| **Officeè½¬æ¢** | LibreOffice | 7.0+ | Officeæ–‡æ¡£è½¬PDF |
| **PDFè§£æ** | MinerU | 2.0+ | PDFè½¬Markdown |
| **æ•°æ®éªŒè¯** | Pydantic | 2.5.0+ | è¯·æ±‚å‚æ•°éªŒè¯ |

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### ç³»ç»Ÿæ¶æ„å›¾

```mermaid
graph TB
    subgraph "ç”¨æˆ·å±‚"
        A[HTTPå®¢æˆ·ç«¯]
        B[Webç•Œé¢]
        C[APIè°ƒç”¨]
    end
    
    subgraph "APIå±‚"
        D[FastAPIåº”ç”¨]
        E[è·¯ç”±å¤„ç†]
        F[è¯·æ±‚éªŒè¯]
        G[å“åº”å¤„ç†]
    end
    
    subgraph "ä»»åŠ¡å¤„ç†å±‚"
        H[TaskProcessor]
        I[ä»»åŠ¡é˜Ÿåˆ—ç³»ç»Ÿ]
        J[å·¥ä½œåç¨‹æ± ]
        K[çŠ¶æ€ç®¡ç†]
    end
    
    subgraph "æœåŠ¡å±‚"
        L[DocumentService]
        M[LibreOfficeè½¬æ¢]
        N[MinerUè½¬æ¢]
        O[æ–‡ä»¶ç®¡ç†]
    end
    
    subgraph "å­˜å‚¨å±‚"
        P[ä»»åŠ¡å­˜å‚¨]
        Q[æ–‡ä»¶ç³»ç»Ÿ]
        R[ä¸´æ—¶å·¥ä½œç©ºé—´]
    end
    
    A --> D
    B --> D
    C --> D
    D --> E
    E --> F
    F --> G
    G --> H
    H --> I
    I --> J
    J --> K
    K --> L
    L --> M
    L --> N
    L --> O
    O --> P
    O --> Q
    O --> R
```

### ä¸‰å±‚æ¶æ„è®¾è®¡

| å±‚çº§ | ç»„ä»¶ | èŒè´£ | æŠ€æœ¯å®ç° |
|------|------|------|----------|
| **APIå±‚** | FastAPI | HTTPæ¥å£å¤„ç†ã€å‚æ•°éªŒè¯ã€å“åº”æ ¼å¼åŒ– | FastAPI + Pydantic |
| **ä»»åŠ¡å¤„ç†å±‚** | TaskProcessor | ä»»åŠ¡è°ƒåº¦ã€çŠ¶æ€ç®¡ç†ã€å¹¶å‘æ§åˆ¶ | asyncio + é˜Ÿåˆ—ç³»ç»Ÿ |
| **æœåŠ¡å±‚** | DocumentService | æ–‡æ¡£è½¬æ¢æ‰§è¡Œã€æ–‡ä»¶æ“ä½œã€é”™è¯¯å¤„ç† | LibreOffice + MinerU |

### æ•°æ®æµå‘

```
HTTPè¯·æ±‚ â†’ APIéªŒè¯ â†’ ä»»åŠ¡åˆ›å»º â†’ é˜Ÿåˆ—è°ƒåº¦ â†’ å¹¶å‘å¤„ç† â†’ æ–‡æ¡£è½¬æ¢ â†’ ç»“æœè¿”å›
```

## ğŸ§© æ ¸å¿ƒç»„ä»¶

### 1. TaskProcessor - ä»»åŠ¡å¤„ç†å™¨

TaskProcessoræ˜¯ç³»ç»Ÿçš„æ ¸å¿ƒè°ƒåº¦å™¨ï¼Œè´Ÿè´£ä»»åŠ¡çš„åˆ›å»ºã€è°ƒåº¦å’Œç®¡ç†ã€‚

#### å…³é”®å±æ€§

```python
class TaskProcessor:
    def __init__(self, max_concurrent_tasks=3, task_check_interval=5):
        # é˜Ÿåˆ—ç³»ç»Ÿ
        self.fetch_queue = asyncio.Queue()           # å¾…è·å–ä»»åŠ¡é˜Ÿåˆ—
        self.task_processing_queue = asyncio.Queue() # ä»»åŠ¡å¤„ç†é˜Ÿåˆ—
        self.update_queue = asyncio.Queue()          # çŠ¶æ€æ›´æ–°é˜Ÿåˆ—
        self.cleanup_queue = asyncio.Queue()         # æ¸…ç†é˜Ÿåˆ—
        self.callback_queue = asyncio.Queue()        # å›è°ƒé˜Ÿåˆ—
        
        # ä»»åŠ¡å­˜å‚¨
        self.tasks: Dict[int, Task] = {}
        self.task_counter = 0
        
        # é…ç½®å‚æ•°
        self.max_concurrent_tasks = max_concurrent_tasks
        self.task_check_interval = task_check_interval
        self.is_running = False
```

#### æ ¸å¿ƒæ–¹æ³•

| æ–¹æ³• | åŠŸèƒ½ | å‚æ•° | è¿”å›å€¼ |
|------|------|------|--------|
| `create_task()` | åˆ›å»ºæ–°ä»»åŠ¡ | task_type, input_path, output_path, params | task_id |
| `get_task_status()` | è·å–ä»»åŠ¡çŠ¶æ€ | task_id | ä»»åŠ¡çŠ¶æ€å­—å…¸ |
| `get_queue_stats()` | è·å–é˜Ÿåˆ—ç»Ÿè®¡ | æ—  | ç»Ÿè®¡ä¿¡æ¯å­—å…¸ |
| `start()` | å¯åŠ¨å¤„ç†å™¨ | æ—  | æ—  |
| `stop()` | åœæ­¢å¤„ç†å™¨ | æ—  | æ—  |

#### ä»»åŠ¡IDç”Ÿæˆç­–ç•¥

ç³»ç»Ÿé‡‡ç”¨UUID4ç”Ÿæˆå”¯ä¸€çš„ä»»åŠ¡æ ‡è¯†ç¬¦ï¼š

```python
import uuid

task_id = str(uuid.uuid4())
# ç¤ºä¾‹: "f47ac10b-58cc-4372-a567-0e02b2c3d479"
```

**ä¼˜åŠ¿ï¼š**
- å…¨å±€å”¯ä¸€æ€§ä¿è¯
- æ— åºåˆ—ä¾èµ–ï¼Œæ”¯æŒåˆ†å¸ƒå¼éƒ¨ç½²
- 128ä½é•¿åº¦ï¼Œç¢°æ’æ¦‚ç‡æä½
- ç¬¦åˆRFC 4122æ ‡å‡†

**æ³¨æ„ï¼š** ç³»ç»Ÿä¸ä½¿ç”¨è‡ªå¢IDï¼Œè€Œæ˜¯ä½¿ç”¨UUIDç¡®ä¿åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹çš„å”¯ä¸€æ€§å’Œå®‰å…¨æ€§ã€‚

#### å·¥ä½œç©ºé—´ç®¡ç†

æ¯ä¸ªä»»åŠ¡éƒ½æœ‰ç‹¬ç«‹çš„å·¥ä½œç©ºé—´ï¼š

```
/app/task_workspace/
â”œâ”€â”€ task_{task_id}/
â”‚   â”œâ”€â”€ input/          # è¾“å…¥æ–‡ä»¶å­˜å‚¨
â”‚   â”œâ”€â”€ output/         # è½¬æ¢ç»“æœå­˜å‚¨
â”‚   â””â”€â”€ temp/           # ä¸´æ—¶æ–‡ä»¶
```

**WorkspaceManageråŠŸèƒ½ï¼š**
- è‡ªåŠ¨åˆ›å»ºä»»åŠ¡å·¥ä½œç›®å½•
- ç®¡ç†æ–‡ä»¶ä¸‹è½½å’Œä¸Šä¼ 
- ä»»åŠ¡å®Œæˆåè‡ªåŠ¨æ¸…ç†
- é˜²æ­¢ç£ç›˜ç©ºé—´æ³„æ¼

#### å·¥ä½œåç¨‹

EnhancedTaskProcessorå¯åŠ¨å¤šä¸ªä¸“ç”¨å·¥ä½œåç¨‹ï¼š

1. **task_worker** (Ã—3): å¹¶å‘å¤„ç†ä»»åŠ¡ï¼Œæ‰§è¡Œå…·ä½“çš„è½¬æ¢æ“ä½œ
2. **update_task_worker**: å¤„ç†ä»»åŠ¡çŠ¶æ€æ›´æ–°
3. **cleanup_worker**: æ¸…ç†ä»»åŠ¡èµ„æºå’Œä¸´æ—¶æ–‡ä»¶
4. **callback_worker**: å¤„ç†ä»»åŠ¡å®Œæˆåçš„å›è°ƒé€šçŸ¥

### 2. S3æœåŠ¡é›†æˆ

ç³»ç»Ÿé›†æˆäº†å®Œæ•´çš„S3æ–‡ä»¶ç®¡ç†åŠŸèƒ½ï¼Œæ”¯æŒæ–‡ä»¶çš„ä¸‹è½½å’Œä¸Šä¼ ã€‚

#### S3DownloadService - æ–‡ä»¶ä¸‹è½½æœåŠ¡

è´Ÿè´£ä»S3å­˜å‚¨æ¡¶ä¸‹è½½è¾“å…¥æ–‡ä»¶åˆ°ä»»åŠ¡å·¥ä½œç©ºé—´ï¼š

```python
class S3DownloadService:
    async def download_file(self, s3_url: str, local_path: str) -> bool:
        """ä»S3ä¸‹è½½æ–‡ä»¶åˆ°æœ¬åœ°è·¯å¾„"""
        # æ”¯æŒæ ¼å¼: s3://bucket-name/path/to/file.pdf
        # ä¸‹è½½åˆ°: /app/task_workspace/task_{id}/input/file.pdf
```

**åŠŸèƒ½ç‰¹æ€§ï¼š**
- æ”¯æŒå¤§æ–‡ä»¶åˆ†å—ä¸‹è½½
- è‡ªåŠ¨é‡è¯•æœºåˆ¶ï¼ˆæœ€å¤š3æ¬¡ï¼‰
- ä¸‹è½½è¿›åº¦ç›‘æ§
- æ–‡ä»¶å®Œæ•´æ€§éªŒè¯
- é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•

#### S3UploadService - æ–‡ä»¶ä¸Šä¼ æœåŠ¡

è´Ÿè´£å°†è½¬æ¢ç»“æœä¸Šä¼ åˆ°S3å­˜å‚¨æ¡¶ï¼š

```python
class S3UploadService:
    async def upload_file(self, local_path: str, s3_key: str) -> str:
        """ä¸Šä¼ æœ¬åœ°æ–‡ä»¶åˆ°S3ï¼Œè¿”å›è®¿é—®URL"""
        # ä¸Šä¼ è·¯å¾„: ai-file/converted/{task_id}/output.md
        # è¿”å›URL: https://ai-file.s3.amazonaws.com/converted/{task_id}/output.md
```

**åŠŸèƒ½ç‰¹æ€§ï¼š**
- æ”¯æŒå¤§æ–‡ä»¶åˆ†å—ä¸Šä¼ 
- è‡ªåŠ¨ç”Ÿæˆè®¿é—®URL
- æ–‡ä»¶å…ƒæ•°æ®è®¾ç½®
- ä¸Šä¼ è¿›åº¦ç›‘æ§
- é”™è¯¯å¤„ç†å’Œé‡è¯•

#### S3é…ç½®

ç³»ç»Ÿé€šè¿‡ç¯å¢ƒå˜é‡é…ç½®S3è®¿é—®ï¼š

```bash
# S3é…ç½®
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key
AWS_DEFAULT_REGION=us-east-1
S3_BUCKET_NAME=ai-file
```

### 3. DocumentService - æ–‡æ¡£è½¬æ¢æœåŠ¡

DocumentServiceå°è£…äº†å…·ä½“çš„æ–‡æ¡£è½¬æ¢é€»è¾‘ï¼Œæ”¯æŒå¤šç§è½¬æ¢ç±»å‹ã€‚

#### æ”¯æŒçš„è½¬æ¢ç±»å‹

| è½¬æ¢ç±»å‹ | è¾“å…¥æ ¼å¼ | è¾“å‡ºæ ¼å¼ | è½¬æ¢å·¥å…· | è¯´æ˜ |
|----------|----------|----------|----------|------|
| **office_to_pdf** | .doc, .docx, .ppt, .pptx, .xls, .xlsx | .pdf | LibreOffice | Officeæ–‡æ¡£è½¬PDF |
| **pdf_to_markdown** | .pdf | .md | MinerU 2.0 | PDFè½¬Markdown |
| **office_to_markdown** | Officeæ ¼å¼ | .md | LibreOffice + MinerU | ç»„åˆè½¬æ¢ |
| **batch_office_to_pdf** | ç›®å½• | ç›®å½• | LibreOffice | æ‰¹é‡Officeè½¬PDF |
| **batch_pdf_to_markdown** | ç›®å½• | ç›®å½• | MinerU 2.0 | æ‰¹é‡PDFè½¬Markdown |
| **batch_office_to_markdown** | ç›®å½• | ç›®å½• | ç»„åˆå·¥å…· | æ‰¹é‡Officeè½¬Markdown |

#### æ ¸å¿ƒæ–¹æ³•

```python
class DocumentService:
    async def convert_office_to_pdf(self, input_path: str, output_path: str) -> Dict[str, Any]:
        """Officeæ–‡æ¡£è½¬PDF"""
        
    async def convert_pdf_to_markdown(self, input_path: str, output_path: str) -> Dict[str, Any]:
        """PDFè½¬Markdown"""
        
    async def convert_office_to_markdown(self, input_path: str, output_path: str) -> Dict[str, Any]:
        """Officeæ–‡æ¡£ç›´æ¥è½¬Markdown"""
        
    async def batch_convert_office_to_markdown(self, input_dir: str, output_dir: str, **kwargs) -> Dict[str, Any]:
        """æ‰¹é‡Officeæ–‡æ¡£è½¬Markdown"""
```

### 3. Task - ä»»åŠ¡æ•°æ®æ¨¡å‹

```python
@dataclass
class Task:
    task_id: int                    # å”¯ä¸€æ ‡è¯†ç¬¦
    task_type: str                  # ä»»åŠ¡ç±»å‹
    status: str                     # å½“å‰çŠ¶æ€ (pending/processing/completed/failed)
    input_path: str                 # è¾“å…¥æ–‡ä»¶è·¯å¾„
    output_path: str                # è¾“å‡ºæ–‡ä»¶è·¯å¾„
    params: Dict[str, Any]          # ä»»åŠ¡å‚æ•°
    priority: str = 'normal'        # ä¼˜å…ˆçº§ (low/normal/high)
    created_at: Optional[datetime] = None    # åˆ›å»ºæ—¶é—´
    started_at: Optional[datetime] = None    # å¼€å§‹å¤„ç†æ—¶é—´
    completed_at: Optional[datetime] = None  # å®Œæˆæ—¶é—´
    error_message: Optional[str] = None      # é”™è¯¯ä¿¡æ¯
    retry_count: int = 0            # é‡è¯•æ¬¡æ•°
    max_retries: int = 3            # æœ€å¤§é‡è¯•æ¬¡æ•°
```

## ğŸ”„ ä»»åŠ¡å¤„ç†é€»è¾‘

### ä»»åŠ¡ç”Ÿå‘½å‘¨æœŸ

```mermaid
stateDiagram-v2
    [*] --> pending: åˆ›å»ºä»»åŠ¡
    pending --> processing: å¼€å§‹å¤„ç†
    processing --> completed: å¤„ç†æˆåŠŸ
    processing --> failed: å¤„ç†å¤±è´¥(é‡è¯•æ¬¡æ•°>=3)
    processing --> pending: å¤„ç†å¤±è´¥(é‡è¯•æ¬¡æ•°<3)
    completed --> [*]
    failed --> [*]
```

### å®Œæ•´ä»»åŠ¡å¤„ç†æµç¨‹å›¾

```mermaid
graph TD
    subgraph "1. ä»»åŠ¡åˆ›å»ºé˜¶æ®µ"
        A[HTTPè¯·æ±‚åˆ°è¾¾] --> B[FastAPIè·¯ç”±å¤„ç†]
        B --> C[å‚æ•°éªŒè¯]
        C --> D[è°ƒç”¨EnhancedTaskProcessor.create_task]
        D --> E[ç”ŸæˆUUID task_id]
        E --> F[åˆ›å»ºDocumentTaskå¯¹è±¡]
        F --> G[å­˜å‚¨åˆ°æ•°æ®åº“]
        G --> H[æ”¾å…¥ä»»åŠ¡é˜Ÿåˆ—]
        H --> I[è¿”å›task_idç»™å®¢æˆ·ç«¯]
    end

    subgraph "2. ä»»åŠ¡è°ƒåº¦é˜¶æ®µ"
        J[task_workeråç¨‹] --> K[ä»æ•°æ®åº“æŸ¥è¯¢pendingä»»åŠ¡]
        K --> L[éªŒè¯ä»»åŠ¡å­˜åœ¨]
        L --> M[æ›´æ–°çŠ¶æ€ä¸ºprocessing]
        M --> N[åˆ›å»ºtask_workspaceç›®å½•]
    end

    subgraph "3. æ–‡ä»¶è·å–é˜¶æ®µ"
        N --> O{è¾“å…¥æºç±»å‹åˆ¤æ–­}
        O -->|S3è·¯å¾„| P[S3DownloadServiceä¸‹è½½]
        O -->|æœ¬åœ°è·¯å¾„| Q[ç›´æ¥è®¿é—®æœ¬åœ°æ–‡ä»¶]
        P --> R[æ–‡ä»¶ä¸‹è½½åˆ°task_workspace]
        Q --> R
        R --> S[éªŒè¯æ–‡ä»¶å®Œæ•´æ€§]
    end

    subgraph "4. ä»»åŠ¡åˆ†å‘å¤„ç†"
        S --> T{ä»»åŠ¡ç±»å‹åˆ¤æ–­}
        T -->|office_to_pdf| U[_process_office_to_pdf]
        T -->|pdf_to_markdown| V[_process_pdf_to_markdown]
        T -->|office_to_markdown| W[_process_office_to_markdown]
        T -->|batch_*| X[æ‰¹é‡å¤„ç†æ–¹æ³•]
    end

    subgraph "5. å…·ä½“è½¬æ¢æ‰§è¡Œ"
        U --> Y[DocumentService.convert_office_to_pdf]
        V --> Z[DocumentService.convert_pdf_to_markdown]
        W --> AA[DocumentService.convert_office_to_markdown]
        X --> BB[DocumentServiceæ‰¹é‡è½¬æ¢æ–¹æ³•]
    end

    subgraph "6. è½¬æ¢å·¥å…·è°ƒç”¨"
        Y --> CC[LibreOfficeå‘½ä»¤è¡Œè½¬æ¢]
        Z --> DD[MinerU 2.0 Python APIè°ƒç”¨]
        W --> EE[LibreOffice + MinerUç»„åˆ]
        BB --> FF[æ‰¹é‡æ–‡ä»¶å¤„ç†å¾ªç¯]
    end

    subgraph "7. ç»“æœä¸Šä¼ é˜¶æ®µ"
        CC --> GG[æ£€æŸ¥è½¬æ¢ç»“æœ]
        DD --> GG
        EE --> GG
        FF --> GG
        GG --> HH{è½¬æ¢æˆåŠŸ?}
        HH -->|æ˜¯| II[S3UploadServiceä¸Šä¼ ç»“æœ]
        II --> JJ[ç”Ÿæˆè®¿é—®URL]
        JJ --> KK[æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºcompleted]
        HH -->|å¦| LL[æ£€æŸ¥é‡è¯•æ¬¡æ•°]
        LL -->|<3æ¬¡| MM[é‡æ–°æ”¾å…¥ä»»åŠ¡é˜Ÿåˆ—]
        LL -->|>=3æ¬¡| NN[æ ‡è®°ä¸ºfailed]
    end

    subgraph "8. èµ„æºæ¸…ç†é˜¶æ®µ"
        KK --> OO[WorkspaceManageræ¸…ç†task_workspace]
        NN --> OO
        MM --> PP[æ¸…ç†ä¸´æ—¶æ–‡ä»¶]
        OO --> QQ[é‡Šæ”¾ç³»ç»Ÿèµ„æº]
        PP --> QQ
        QQ --> RR[ä»»åŠ¡å®Œæˆ]
    end

    H --> J
    MM --> J

    classDef creation fill:#e1f5fe
    classDef scheduling fill:#f3e5f5
    classDef download fill:#e3f2fd
    classDef execution fill:#e8f5e8
    classDef dispatch fill:#fff3e0
    classDef conversion fill:#ffebee
    classDef tools fill:#f1f8e9
    classDef upload fill:#e0f2f1
    classDef cleanup fill:#f9fbe7

    class A,B,C,D,E,F,G,H,I creation
    class J,K,L,M,N scheduling
    class O,P,Q,R,S download
    class T,U,V,W,X dispatch
    class Y,Z,AA,BB conversion
    class CC,DD,EE,FF tools
    class GG,HH,II,JJ,KK,LL,MM,NN upload
    class OO,PP,QQ,RR cleanup

## è¯¦ç»†ä»»åŠ¡æ‰§è¡Œæµç¨‹

### 1. ä»»åŠ¡åˆ›å»ºå’Œåˆå§‹åŒ–

```python
# 1. æ¥æ”¶HTTPè¯·æ±‚
POST /api/tasks/create
{
    "task_type": "pdf_to_markdown",
    "input_path": "s3://ai-file/document.pdf",
    "platform": "gaojiaqi",
    "priority": "normal"
}

# 2. ç”ŸæˆUUIDä»»åŠ¡ID
task_id = str(uuid.uuid4())  # ä¾‹å¦‚: "f47ac10b-58cc-4372-a567-0e02b2c3d479"

# 3. åˆ›å»ºæ•°æ®åº“è®°å½•
task = DocumentTask(
    id=task_id,
    task_type="pdf_to_markdown",
    input_path="s3://ai-file/document.pdf",
    status=TaskStatus.pending,
    platform="gaojiaqi",
    priority=TaskPriority.normal
)
```

### 2. å·¥ä½œç©ºé—´åˆ›å»º

```python
# åˆ›å»ºä»»åŠ¡ä¸“ç”¨å·¥ä½œç©ºé—´
workspace_path = f"/app/task_workspace/task_{task_id}"
workspace_manager.create_workspace(workspace_path)

# ç›®å½•ç»“æ„:
# /app/task_workspace/task_f47ac10b-58cc-4372-a567-0e02b2c3d479/
# â”œâ”€â”€ input/          # è¾“å…¥æ–‡ä»¶å­˜å‚¨
# â”œâ”€â”€ output/         # è½¬æ¢ç»“æœå­˜å‚¨
# â””â”€â”€ temp/           # ä¸´æ—¶æ–‡ä»¶
```

### 3. æ–‡ä»¶ä¸‹è½½é˜¶æ®µ

```python
# S3æ–‡ä»¶ä¸‹è½½
if input_path.startswith("s3://"):
    s3_service = S3DownloadService()
    local_input_path = f"{workspace_path}/input/document.pdf"
    success = await s3_service.download_file(input_path, local_input_path)

    # ä¸‹è½½æ—¥å¿—ç¤ºä¾‹:
    # ğŸ“¥ å¼€å§‹ä¸‹è½½: s3://ai-file/document.pdf
    # ğŸ“Š æ–‡ä»¶å¤§å°: 167MB
    # â±ï¸ ä¸‹è½½è€—æ—¶: 15.3ç§’
    # âœ… ä¸‹è½½å®Œæˆ: /app/task_workspace/task_f47ac10b.../input/document.pdf
```

### 4. æ–‡æ¡£è½¬æ¢é˜¶æ®µ

```python
# PDFè½¬Markdownè½¬æ¢
document_service = DocumentService()
result = await document_service.convert_pdf_to_markdown(
    input_path=local_input_path,
    output_path=f"{workspace_path}/output/document.md"
)

# MinerU 2.0è½¬æ¢æ—¥å¿—ç¤ºä¾‹:
# ğŸ¤– å¯åŠ¨MinerU 2.0 AIè½¬æ¢
# ğŸ“„ æ–‡æ¡£é¡µæ•°: 268é¡µ
# ğŸ” Layout Predict: 100% (268/268)
# ğŸ§® MFD Predict: 100% (268/268)
# ğŸ“ MFR Predict: 100% (814/814)
# âœ… è½¬æ¢å®Œæˆ: document.md (2.3MB)
```

### 5. ç»“æœä¸Šä¼ é˜¶æ®µ

```python
# ä¸Šä¼ è½¬æ¢ç»“æœåˆ°S3
s3_upload_service = S3UploadService()
output_s3_key = f"converted/{task_id}/document.md"
s3_url = await s3_upload_service.upload_file(
    local_path=f"{workspace_path}/output/document.md",
    s3_key=output_s3_key
)

# ä¸Šä¼ æ—¥å¿—ç¤ºä¾‹:
# ğŸ“¤ å¼€å§‹ä¸Šä¼ : document.md
# ğŸ“Š æ–‡ä»¶å¤§å°: 2.3MB
# â±ï¸ ä¸Šä¼ è€—æ—¶: 3.2ç§’
# âœ… ä¸Šä¼ å®Œæˆ: https://ai-file.s3.amazonaws.com/converted/f47ac10b.../document.md
```

### 6. ä»»åŠ¡å®Œæˆå’Œæ¸…ç†

```python
# æ›´æ–°ä»»åŠ¡çŠ¶æ€
task.status = TaskStatus.completed
task.output_path = f"{workspace_path}/output/document.md"
task.s3_urls = [s3_url]
task.completed_at = datetime.utcnow()

# æ¸…ç†å·¥ä½œç©ºé—´
workspace_manager.cleanup_workspace(workspace_path)

# æ¸…ç†æ—¥å¿—ç¤ºä¾‹:
# ğŸ§¹ å¼€å§‹æ¸…ç†å·¥ä½œç©ºé—´: task_f47ac10b...
# ğŸ—‘ï¸ åˆ é™¤è¾“å…¥æ–‡ä»¶: document.pdf (167MB)
# ğŸ—‘ï¸ åˆ é™¤è¾“å‡ºæ–‡ä»¶: document.md (2.3MB)
# ğŸ—‘ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶: 15ä¸ªæ–‡ä»¶ (45MB)
# âœ… å·¥ä½œç©ºé—´æ¸…ç†å®Œæˆï¼Œé‡Šæ”¾215MBç£ç›˜ç©ºé—´
```
```

### è¯¦ç»†å¤„ç†æ­¥éª¤è¯´æ˜

#### é˜¶æ®µ1: ä»»åŠ¡åˆ›å»º (APIå±‚)

**æ­¥éª¤1.1: HTTPè¯·æ±‚å¤„ç†**
```python
@app.post("/api/tasks")
async def create_task(request: TaskCreateRequest):
    """
    å¤„ç†ä»»åŠ¡åˆ›å»ºè¯·æ±‚
    1. æ¥æ”¶HTTP POSTè¯·æ±‚
    2. è§£æJSONè¯·æ±‚ä½“
    3. éªŒè¯è¯·æ±‚æ ¼å¼
    """
    if not task_processor:
        raise HTTPException(status_code=503, detail="Task processor not available")
```

**æ­¥éª¤1.2: å‚æ•°éªŒè¯**
```python
    # éªŒè¯ä»»åŠ¡ç±»å‹
    valid_task_types = {
        'office_to_pdf', 'pdf_to_markdown', 'office_to_markdown',
        'batch_office_to_pdf', 'batch_pdf_to_markdown', 'batch_office_to_markdown'
    }

    if request.task_type not in valid_task_types:
        raise HTTPException(status_code=400, detail=f"Invalid task type: {request.task_type}")

    # éªŒè¯æ–‡ä»¶è·¯å¾„
    if not Path(request.input_path).exists():
        raise HTTPException(status_code=400, detail="Input path does not exist")
```

**æ­¥éª¤1.3: è°ƒç”¨ä»»åŠ¡å¤„ç†å™¨**
```python
    task_id = await task_processor.create_task(
        task_type=request.task_type,
        input_path=request.input_path,
        output_path=request.output_path,
        params=request.params,
        priority=request.priority
    )

    return TaskResponse(task_id=task_id, message=f"Task {task_id} created successfully")
```

#### é˜¶æ®µ2: ä»»åŠ¡è°ƒåº¦ (TaskProcessorå±‚)

**æ­¥éª¤2.1: ä»»åŠ¡å¯¹è±¡åˆ›å»º**
```python
async def create_task(self, task_type: str, input_path: str, output_path: str,
                     params: Dict[str, Any] = None, priority: str = 'normal') -> int:
    """
    åˆ›å»ºä»»åŠ¡å¯¹è±¡å¹¶åŠ å…¥è°ƒåº¦é˜Ÿåˆ—
    """
    # ç”Ÿæˆå”¯ä¸€ä»»åŠ¡ID
    async with self.task_lock:
        self.task_counter += 1
        task_id = self.task_counter

    # åˆ›å»ºTaskå¯¹è±¡
    task = Task(
        task_id=task_id,
        task_type=task_type,
        status='pending',
        input_path=input_path,
        output_path=output_path,
        params=params or {},
        priority=priority,
        created_at=datetime.now(),
        retry_count=0,
        max_retries=3
    )

    # å­˜å‚¨ä»»åŠ¡
    self.tasks[task_id] = task
    self.logger.info(f"Created task {task_id}: {task_type}")

    # æ”¾å…¥è·å–é˜Ÿåˆ—
    await self.fetch_queue.put(task_id)

    return task_id
```

**æ­¥éª¤2.2: fetch_task_workerå¤„ç†**
```python
async def _fetch_task_worker(self):
    """
    è·å–ä»»åŠ¡å·¥ä½œåç¨‹
    è´Ÿè´£å°†æ–°ä»»åŠ¡ä»fetch_queueè½¬ç§»åˆ°task_processing_queue
    """
    while self.is_running:
        try:
            # ä»è·å–é˜Ÿåˆ—è·å–ä»»åŠ¡ID
            task_id = await asyncio.wait_for(
                self.fetch_queue.get(),
                timeout=self.task_check_interval
            )

            # éªŒè¯ä»»åŠ¡å­˜åœ¨
            task = self.tasks.get(task_id)
            if not task:
                self.logger.warning(f"Task {task_id} not found in tasks dict")
                continue

            # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
            if task.status != 'pending':
                self.logger.warning(f"Task {task_id} status is {task.status}, skipping")
                continue

            # è½¬ç§»åˆ°å¤„ç†é˜Ÿåˆ—
            await self.task_processing_queue.put(task_id)
            self.logger.debug(f"Task {task_id} moved to processing queue")

        except asyncio.TimeoutError:
            continue
        except Exception as e:
            self.logger.error(f"Error in fetch_task_worker: {e}")

#### é˜¶æ®µ3: ä»»åŠ¡æ‰§è¡Œ (å¹¶å‘å¤„ç†)

**æ­¥éª¤3.1: task_workeråç¨‹å¤„ç†**
```python
async def _task_worker(self, worker_id: int):
    """
    ä»»åŠ¡å·¥ä½œåç¨‹ (å¹¶å‘è¿è¡Œ3ä¸ª)
    è´Ÿè´£å®é™…çš„ä»»åŠ¡å¤„ç†
    """
    self.logger.info(f"Task worker {worker_id} started")

    while self.is_running:
        try:
            # ä»å¤„ç†é˜Ÿåˆ—è·å–ä»»åŠ¡
            task_id = await asyncio.wait_for(
                self.task_processing_queue.get(),
                timeout=self.task_check_interval
            )

            task = self.tasks.get(task_id)
            if not task:
                self.logger.warning(f"Task {task_id} not found")
                continue

            self.logger.info(f"Worker {worker_id} processing task {task_id}")

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤„ç†ä¸­
            task.status = 'processing'
            task.started_at = datetime.now()
            await self.update_queue.put(task_id)

            # åˆ›å»ºä»»åŠ¡å·¥ä½œç©ºé—´
            task_workspace = self.workspace_dir / f"task_{task_id}"
            task_workspace.mkdir(parents=True, exist_ok=True)

            # æ‰§è¡Œä»»åŠ¡å¤„ç†
            result = await self._process_task(task, task_workspace)

            # å¤„ç†æ‰§è¡Œç»“æœ
            await self._handle_task_result(task, result)

        except asyncio.TimeoutError:
            continue
        except Exception as e:
            self.logger.error(f"Error in task_worker {worker_id}: {e}")
            # å¤„ç†å¼‚å¸¸æƒ…å†µ
            if 'task' in locals():
                await self._handle_task_error(task, str(e))
```

**æ­¥éª¤3.2: ä»»åŠ¡å¤„ç†åˆ†å‘**
```python
async def _process_task(self, task: Task, workspace: Path) -> Dict[str, Any]:
    """
    æ ¹æ®ä»»åŠ¡ç±»å‹åˆ†å‘åˆ°å…·ä½“çš„å¤„ç†æ–¹æ³•
    """
    try:
        self.logger.info(f"Processing task {task.task_id} of type {task.task_type}")

        # ä»»åŠ¡ç±»å‹åˆ†å‘
        if task.task_type == 'office_to_pdf':
            return await self._process_office_to_pdf(task, workspace)
        elif task.task_type == 'pdf_to_markdown':
            return await self._process_pdf_to_markdown(task, workspace)
        elif task.task_type == 'office_to_markdown':
            return await self._process_office_to_markdown(task, workspace)
        elif task.task_type == 'batch_office_to_pdf':
            return await self._process_batch_office_to_pdf(task, workspace)
        elif task.task_type == 'batch_pdf_to_markdown':
            return await self._process_batch_pdf_to_markdown(task, workspace)
        elif task.task_type == 'batch_office_to_markdown':
            return await self._process_batch_office_to_markdown(task, workspace)
        else:
            raise ValueError(f"Unsupported task type: {task.task_type}")

    except Exception as e:
        self.logger.error(f"Error processing task {task.task_id}: {e}")
        return {
            'success': False,
            'error': str(e),
            'error_type': type(e).__name__
        }
```

#### é˜¶æ®µ4: å…·ä½“è½¬æ¢å¤„ç†

**æ­¥éª¤4.1: Officeè½¬PDFå¤„ç†**
```python
async def _process_office_to_pdf(self, task: Task, workspace: Path) -> Dict[str, Any]:
    """
    å¤„ç†Officeæ–‡æ¡£è½¬PDFä»»åŠ¡
    """
    try:
        self.logger.info(f"Converting Office to PDF: {task.input_path}")

        # éªŒè¯è¾“å…¥æ–‡ä»¶
        input_path = Path(task.input_path)
        if not input_path.exists():
            raise FileNotFoundError(f"Input file not found: {task.input_path}")

        # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
        if input_path.suffix.lower() not in self.doc_service.office_formats:
            raise ValueError(f"Unsupported file format: {input_path.suffix}")

        # å‡†å¤‡è¾“å‡ºè·¯å¾„
        output_path = Path(task.output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # è°ƒç”¨DocumentServiceè¿›è¡Œè½¬æ¢
        result = await self.doc_service.convert_office_to_pdf(
            input_path=str(input_path),
            output_path=str(output_path)
        )

        if result['success']:
            # éªŒè¯è¾“å‡ºæ–‡ä»¶
            if not output_path.exists():
                raise FileNotFoundError("Output PDF file was not created")

            file_size = output_path.stat().st_size
            if file_size == 0:
                raise ValueError("Output PDF file is empty")

            self.logger.info(f"Office to PDF conversion completed: {file_size} bytes")

            return {
                'success': True,
                'input_file': str(input_path),
                'output_file': str(output_path),
                'file_size': file_size,
                'conversion_type': 'office_to_pdf'
            }
        else:
            raise Exception(result.get('error', 'LibreOffice conversion failed'))

    except Exception as e:
        self.logger.error(f"Office to PDF conversion failed: {e}")
        raise
```

**æ­¥éª¤4.2: PDFè½¬Markdownå¤„ç†**
```python
async def _process_pdf_to_markdown(self, task: Task, workspace: Path) -> Dict[str, Any]:
    """
    å¤„ç†PDFè½¬Markdownä»»åŠ¡
    """
    try:
        self.logger.info(f"Converting PDF to Markdown: {task.input_path}")

        # éªŒè¯è¾“å…¥æ–‡ä»¶
        input_path = Path(task.input_path)
        if not input_path.exists():
            raise FileNotFoundError(f"Input file not found: {task.input_path}")

        if input_path.suffix.lower() != '.pdf':
            raise ValueError(f"Expected PDF file, got: {input_path.suffix}")

        # å‡†å¤‡è¾“å‡ºè·¯å¾„
        output_path = Path(task.output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # æ¸…ç†GPUå†…å­˜ (MinerUéœ€è¦å¤§é‡GPUå†…å­˜)
        self._clear_gpu_memory()

        # è°ƒç”¨DocumentServiceè¿›è¡Œè½¬æ¢
        result = await self.doc_service.convert_pdf_to_markdown(
            input_path=str(input_path),
            output_path=str(output_path),
            **task.params
        )

        if result['success']:
            # éªŒè¯è¾“å‡ºæ–‡ä»¶
            if not output_path.exists():
                raise FileNotFoundError("Output Markdown file was not created")

            file_size = output_path.stat().st_size
            if file_size == 0:
                raise ValueError("Output Markdown file is empty")

            self.logger.info(f"PDF to Markdown conversion completed: {file_size} bytes")

            return {
                'success': True,
                'input_file': str(input_path),
                'output_file': str(output_path),
                'file_size': file_size,
                'conversion_type': 'pdf_to_markdown',
                'pages_processed': result.get('pages_processed', 0)
            }
        else:
            raise Exception(result.get('error', 'MinerU conversion failed'))

    except Exception as e:
        self.logger.error(f"PDF to Markdown conversion failed: {e}")
        # æ¸…ç†GPUå†…å­˜
        self._clear_gpu_memory()
        raise

**æ­¥éª¤4.3: Officeè½¬Markdownç»„åˆå¤„ç†**
```python
async def _process_office_to_markdown(self, task: Task, workspace: Path) -> Dict[str, Any]:
    """
    å¤„ç†Officeæ–‡æ¡£ç›´æ¥è½¬Markdownä»»åŠ¡ (ç»„åˆè½¬æ¢)
    æ­¥éª¤: Office â†’ PDF â†’ Markdown
    """
    try:
        self.logger.info(f"Converting Office to Markdown: {task.input_path}")

        input_path = Path(task.input_path)
        output_path = Path(task.output_path)

        # ç¬¬ä¸€æ­¥: Officeè½¬PDF (ä¸´æ—¶æ–‡ä»¶)
        temp_pdf = workspace / f"temp_{task.task_id}.pdf"

        self.logger.info(f"Step 1: Converting Office to PDF (temp: {temp_pdf})")
        pdf_result = await self.doc_service.convert_office_to_pdf(
            input_path=str(input_path),
            output_path=str(temp_pdf)
        )

        if not pdf_result['success']:
            raise Exception(f"Office to PDF failed: {pdf_result.get('error')}")

        # éªŒè¯ä¸´æ—¶PDFæ–‡ä»¶
        if not temp_pdf.exists() or temp_pdf.stat().st_size == 0:
            raise Exception("Temporary PDF file creation failed")

        # ç¬¬äºŒæ­¥: PDFè½¬Markdown
        self.logger.info(f"Step 2: Converting PDF to Markdown")
        markdown_result = await self.doc_service.convert_pdf_to_markdown(
            input_path=str(temp_pdf),
            output_path=str(output_path),
            **task.params
        )

        if not markdown_result['success']:
            raise Exception(f"PDF to Markdown failed: {markdown_result.get('error')}")

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            temp_pdf.unlink()
            self.logger.debug(f"Cleaned up temporary file: {temp_pdf}")
        except Exception as e:
            self.logger.warning(f"Failed to clean up temp file {temp_pdf}: {e}")

        # éªŒè¯æœ€ç»ˆè¾“å‡º
        if not output_path.exists():
            raise FileNotFoundError("Final Markdown file was not created")

        file_size = output_path.stat().st_size
        self.logger.info(f"Office to Markdown conversion completed: {file_size} bytes")

        return {
            'success': True,
            'input_file': str(input_path),
            'output_file': str(output_path),
            'file_size': file_size,
            'conversion_type': 'office_to_markdown',
            'temp_pdf_size': pdf_result.get('file_size', 0),
            'pages_processed': markdown_result.get('pages_processed', 0)
        }

    except Exception as e:
        self.logger.error(f"Office to Markdown conversion failed: {e}")
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            if 'temp_pdf' in locals() and temp_pdf.exists():
                temp_pdf.unlink()
        except:
            pass
        raise
```

**æ­¥éª¤4.4: æ‰¹é‡å¤„ç†**
```python
async def _process_batch_office_to_markdown(self, task: Task, workspace: Path) -> Dict[str, Any]:
    """
    å¤„ç†æ‰¹é‡Officeè½¬Markdownä»»åŠ¡
    """
    try:
        self.logger.info(f"Batch converting Office to Markdown: {task.input_path}")

        input_dir = Path(task.input_path)
        output_dir = Path(task.output_path)

        if not input_dir.exists() or not input_dir.is_dir():
            raise ValueError(f"Input directory does not exist: {input_dir}")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir.mkdir(parents=True, exist_ok=True)

        # è·å–å‚æ•°
        recursive = task.params.get('recursive', False)
        force_reprocess = task.params.get('force_reprocess', False)

        # æ‰«æè¾“å…¥æ–‡ä»¶
        pattern = "**/*" if recursive else "*"
        office_files = []

        for ext in self.doc_service.office_formats:
            office_files.extend(input_dir.glob(f"{pattern}{ext}"))

        if not office_files:
            raise ValueError(f"No Office files found in {input_dir}")

        self.logger.info(f"Found {len(office_files)} Office files to process")

        # æ‰¹é‡å¤„ç†ç»Ÿè®¡
        results = {
            'total_files': len(office_files),
            'successful': 0,
            'failed': 0,
            'skipped': 0,
            'processed_files': [],
            'failed_files': [],
            'skipped_files': []
        }

        # é€ä¸ªå¤„ç†æ–‡ä»¶
        for i, office_file in enumerate(office_files, 1):
            try:
                # è®¡ç®—ç›¸å¯¹è·¯å¾„å’Œè¾“å‡ºè·¯å¾„
                rel_path = office_file.relative_to(input_dir)
                output_file = output_dir / rel_path.with_suffix('.md')

                # æ£€æŸ¥æ˜¯å¦éœ€è¦è·³è¿‡
                if output_file.exists() and not force_reprocess:
                    self.logger.info(f"Skipping existing file ({i}/{len(office_files)}): {rel_path}")
                    results['skipped'] += 1
                    results['skipped_files'].append(str(rel_path))
                    continue

                self.logger.info(f"Processing file ({i}/{len(office_files)}): {rel_path}")

                # åˆ›å»ºè¾“å‡ºç›®å½•
                output_file.parent.mkdir(parents=True, exist_ok=True)

                # è½¬æ¢å•ä¸ªæ–‡ä»¶
                file_result = await self.doc_service.convert_office_to_markdown(
                    input_path=str(office_file),
                    output_path=str(output_file)
                )

                if file_result['success']:
                    results['successful'] += 1
                    results['processed_files'].append({
                        'input': str(rel_path),
                        'output': str(output_file.relative_to(output_dir)),
                        'size': output_file.stat().st_size if output_file.exists() else 0
                    })
                    self.logger.info(f"Successfully processed: {rel_path}")
                else:
                    raise Exception(file_result.get('error', 'Conversion failed'))

            except Exception as e:
                results['failed'] += 1
                results['failed_files'].append({
                    'file': str(rel_path) if 'rel_path' in locals() else str(office_file),
                    'error': str(e)
                })
                self.logger.error(f"Failed to process {office_file}: {e}")
                continue

        # ç”Ÿæˆå¤„ç†æŠ¥å‘Š
        success_rate = (results['successful'] / results['total_files']) * 100
        self.logger.info(f"Batch processing completed: {results['successful']}/{results['total_files']} files ({success_rate:.1f}%)")

        return {
            'success': True,
            'conversion_type': 'batch_office_to_markdown',
            'input_directory': str(input_dir),
            'output_directory': str(output_dir),
            'statistics': results,
            'success_rate': success_rate
        }

    except Exception as e:
        self.logger.error(f"Batch Office to Markdown conversion failed: {e}")
        raise

#### é˜¶æ®µ5: ç»“æœå¤„ç†å’ŒçŠ¶æ€æ›´æ–°

**æ­¥éª¤5.1: å¤„ç†ä»»åŠ¡ç»“æœ**
```python
async def _handle_task_result(self, task: Task, result: Dict[str, Any]):
    """
    å¤„ç†ä»»åŠ¡æ‰§è¡Œç»“æœ
    """
    try:
        if result['success']:
            # æˆåŠŸå¤„ç†
            task.status = 'completed'
            task.completed_at = datetime.now()
            task.result = result

            self.logger.info(f"Task {task.task_id} completed successfully")

            # è®°å½•å¤„ç†ç»Ÿè®¡
            processing_time = (task.completed_at - task.started_at).total_seconds()
            self.logger.info(f"Task {task.task_id} processing time: {processing_time:.2f}s")

        else:
            # å¤±è´¥å¤„ç†
            await self._handle_task_error(task, result.get('error', 'Unknown error'))

    except Exception as e:
        self.logger.error(f"Error handling task result for task {task.task_id}: {e}")
        await self._handle_task_error(task, str(e))

    finally:
        # æ”¾å…¥æ›´æ–°é˜Ÿåˆ—å’Œæ¸…ç†é˜Ÿåˆ—
        await self.update_queue.put(task.task_id)
        await self.cleanup_queue.put(task.task_id)
```

**æ­¥éª¤5.2: é”™è¯¯å¤„ç†å’Œé‡è¯•é€»è¾‘**
```python
async def _handle_task_error(self, task: Task, error_message: str):
    """
    å¤„ç†ä»»åŠ¡é”™è¯¯å’Œé‡è¯•é€»è¾‘
    """
    task.error_message = error_message
    task.retry_count += 1

    # åˆ†æé”™è¯¯ç±»å‹
    error_analysis = self._analyze_error(error_message)
    task.error_analysis = error_analysis

    self.logger.error(f"Task {task.task_id} failed (attempt {task.retry_count}): {error_message}")
    self.logger.info(f"Error analysis: {error_analysis}")

    # åˆ¤æ–­æ˜¯å¦é‡è¯•
    if task.retry_count < task.max_retries and self._should_retry(error_message):
        # é‡è¯•é€»è¾‘
        task.status = 'pending'

        # è®¡ç®—é‡è¯•å»¶è¿Ÿ (æŒ‡æ•°é€€é¿)
        retry_delay = min(2 ** (task.retry_count - 1), 60)  # æœ€å¤§60ç§’

        self.logger.info(f"Retrying task {task.task_id} in {retry_delay} seconds")

        # å»¶è¿Ÿåé‡æ–°æ”¾å…¥é˜Ÿåˆ—
        asyncio.create_task(self._delayed_retry(task.task_id, retry_delay))

    else:
        # æ ‡è®°ä¸ºæœ€ç»ˆå¤±è´¥
        task.status = 'failed'
        task.completed_at = datetime.now()

        self.logger.error(f"Task {task.task_id} failed permanently after {task.retry_count} attempts")

async def _delayed_retry(self, task_id: int, delay: float):
    """
    å»¶è¿Ÿé‡è¯•ä»»åŠ¡
    """
    await asyncio.sleep(delay)
    await self.fetch_queue.put(task_id)
    self.logger.info(f"Task {task_id} re-queued for retry")

def _should_retry(self, error_message: str) -> bool:
    """
    åˆ¤æ–­é”™è¯¯æ˜¯å¦åº”è¯¥é‡è¯•
    """
    # ä¸é‡è¯•çš„é”™è¯¯ç±»å‹
    non_retryable_errors = [
        'FileNotFoundError',
        'Permission denied',
        'Invalid file format',
        'Unsupported task type'
    ]

    for error_type in non_retryable_errors:
        if error_type in error_message:
            return False

    return True

def _analyze_error(self, error_message: str) -> str:
    """
    åˆ†æé”™è¯¯ç±»å‹å¹¶æä¾›è§£å†³å»ºè®®
    """
    if "CUDA out of memory" in error_message:
        return "GPUå†…å­˜ä¸è¶³ - å»ºè®®æ¸…ç†GPUå†…å­˜æˆ–å‡å°‘å¹¶å‘ä»»åŠ¡æ•°"
    elif "FileNotFoundError" in error_message:
        return "æ–‡ä»¶æœªæ‰¾åˆ° - æ£€æŸ¥è¾“å…¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®"
    elif "Permission denied" in error_message:
        return "æƒé™é”™è¯¯ - æ£€æŸ¥æ–‡ä»¶å’Œç›®å½•çš„è¯»å†™æƒé™"
    elif "LibreOffice" in error_message:
        return "LibreOfficeè½¬æ¢é”™è¯¯ - æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’ŒLibreOfficeçŠ¶æ€"
    elif "MinerU" in error_message:
        return "MinerUè½¬æ¢é”™è¯¯ - æ£€æŸ¥GPUçŠ¶æ€å’Œæ¨¡å‹åŠ è½½"
    else:
        return f"æœªçŸ¥é”™è¯¯ - {error_message[:100]}..."
```

#### é˜¶æ®µ6: åç»­å¤„ç† (æ¸…ç†å’Œå›è°ƒ)

**æ­¥éª¤6.1: update_task_workerå¤„ç†**
```python
async def _update_task_worker(self):
    """
    ä»»åŠ¡çŠ¶æ€æ›´æ–°å·¥ä½œåç¨‹
    """
    while self.is_running:
        try:
            task_id = await asyncio.wait_for(
                self.update_queue.get(),
                timeout=self.task_check_interval
            )

            task = self.tasks.get(task_id)
            if not task:
                continue

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€åˆ°æ•°æ®åº“æˆ–ç¼“å­˜
            await self._persist_task_state(task)

            # å‘é€çŠ¶æ€æ›´æ–°é€šçŸ¥
            await self._notify_task_update(task)

        except asyncio.TimeoutError:
            continue
        except Exception as e:
            self.logger.error(f"Error in update_task_worker: {e}")
```

**æ­¥éª¤6.2: cleanup_workerå¤„ç†**
```python
async def _cleanup_worker(self):
    """
    èµ„æºæ¸…ç†å·¥ä½œåç¨‹
    """
    while self.is_running:
        try:
            task_id = await asyncio.wait_for(
                self.cleanup_queue.get(),
                timeout=self.task_check_interval
            )

            # æ¸…ç†ä»»åŠ¡å·¥ä½œç©ºé—´
            task_workspace = self.workspace_dir / f"task_{task_id}"
            if task_workspace.exists():
                shutil.rmtree(task_workspace)
                self.logger.debug(f"Cleaned up workspace for task {task_id}")

            # æ¸…ç†GPUå†…å­˜ (å¦‚æœæ˜¯GPUä»»åŠ¡)
            task = self.tasks.get(task_id)
            if task and 'pdf_to_markdown' in task.task_type:
                self._clear_gpu_memory()

            # æ”¾å…¥å›è°ƒé˜Ÿåˆ—
            await self.callback_queue.put(task_id)

        except asyncio.TimeoutError:
            continue
        except Exception as e:
            self.logger.error(f"Error in cleanup_worker: {e}")
```

**æ­¥éª¤6.3: callback_workerå¤„ç†**
```python
async def _callback_worker(self):
    """
    å›è°ƒå¤„ç†å·¥ä½œåç¨‹
    """
    while self.is_running:
        try:
            task_id = await asyncio.wait_for(
                self.callback_queue.get(),
                timeout=self.task_check_interval
            )

            task = self.tasks.get(task_id)
            if not task:
                continue

            # æ‰§è¡Œä»»åŠ¡å®Œæˆå›è°ƒ
            await self._execute_task_callback(task)

            # æ¸…ç†æ—§ä»»åŠ¡ (å¯é€‰)
            await self._cleanup_old_tasks()

        except asyncio.TimeoutError:
            continue
        except Exception as e:
            self.logger.error(f"Error in callback_worker: {e}")

async def _execute_task_callback(self, task: Task):
    """
    æ‰§è¡Œä»»åŠ¡å®Œæˆå›è°ƒ
    """
    try:
        # å‘é€å®Œæˆé€šçŸ¥
        if task.status == 'completed':
            self.logger.info(f"Task {task.task_id} completed successfully")
            # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ webhooké€šçŸ¥ã€é‚®ä»¶é€šçŸ¥ç­‰
        elif task.status == 'failed':
            self.logger.error(f"Task {task.task_id} failed: {task.error_message}")
            # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ å¤±è´¥å‘Šè­¦

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self._update_statistics(task)

    except Exception as e:
        self.logger.error(f"Error executing callback for task {task.task_id}: {e}")
```

### å…³é”®å¤„ç†ç»†èŠ‚æ€»ç»“

#### ğŸ”§ **å·¥ä½œç©ºé—´ç®¡ç†**
- æ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„å·¥ä½œç›®å½• `/app/task_workspace/task_{id}/`
- ä¸´æ—¶æ–‡ä»¶è‡ªåŠ¨æ¸…ç†ï¼Œé¿å…ç£ç›˜ç©ºé—´æµªè´¹
- æ”¯æŒå¹¶å‘ä»»åŠ¡çš„æ–‡ä»¶éš”ç¦»

#### ğŸ”„ **çŠ¶æ€è½¬æ¢æ§åˆ¶**
- ä¸¥æ ¼çš„çŠ¶æ€è½¬æ¢éªŒè¯ï¼š`pending â†’ processing â†’ completed/failed`
- é‡è¯•ä»»åŠ¡é‡æ–°å›åˆ° `pending` çŠ¶æ€
- çŠ¶æ€æ›´æ–°é€šè¿‡ä¸“ç”¨é˜Ÿåˆ—å¼‚æ­¥å¤„ç†

#### âš¡ **æ€§èƒ½ä¼˜åŒ–æªæ–½**
- GPUå†…å­˜è‡ªåŠ¨æ¸…ç†ï¼Œé˜²æ­¢OOM
- æŒ‡æ•°é€€é¿é‡è¯•ç­–ç•¥ï¼Œé¿å…ç³»ç»Ÿè¿‡è½½
- æ‰¹é‡å¤„ç†æ”¯æŒè·³è¿‡å·²å­˜åœ¨æ–‡ä»¶

#### ğŸ›¡ï¸ **é”™è¯¯å¤„ç†ç­–ç•¥**
- è¯¦ç»†çš„é”™è¯¯åˆ†æå’Œåˆ†ç±»
- æ™ºèƒ½é‡è¯•åˆ¤æ–­ï¼Œé¿å…æ— æ•ˆé‡è¯•
- å®Œæ•´çš„é”™è¯¯æ—¥å¿—è®°å½•

#### ğŸ“Š **ç›‘æ§å’Œç»Ÿè®¡**
- å®æ—¶ä»»åŠ¡çŠ¶æ€è·Ÿè¸ª
- å¤„ç†æ—¶é—´ç»Ÿè®¡
- æˆåŠŸç‡å’Œå¤±è´¥ç‡ç›‘æ§
```
```
```

## ğŸ“Š é˜Ÿåˆ—ç³»ç»Ÿ

### é˜Ÿåˆ—æ¶æ„

```mermaid
graph LR
    subgraph "ä»»åŠ¡æµè½¬"
        A[fetch_queue] --> B[task_processing_queue]
        B --> C[update_queue]
        C --> D[cleanup_queue]
        D --> E[callback_queue]
    end
    
    subgraph "å·¥ä½œåç¨‹"
        F[fetch_worker] --> A
        G[task_worker_1] --> B
        H[task_worker_2] --> B
        I[task_worker_3] --> B
        J[update_worker] --> C
        K[cleanup_worker] --> D
        L[callback_worker] --> E
    end
```

### é˜Ÿåˆ—è¯¦ç»†è¯´æ˜

| é˜Ÿåˆ—åç§° | ä½œç”¨ | æ•°æ®ç±»å‹ | å¤„ç†åç¨‹ | è¯´æ˜ |
|----------|------|----------|----------|------|
| **fetch_queue** | å­˜å‚¨æ–°åˆ›å»ºçš„ä»»åŠ¡ID | int | fetch_task_worker | ä»»åŠ¡å…¥å£é˜Ÿåˆ— |
| **task_processing_queue** | å­˜å‚¨å¾…å¤„ç†çš„ä»»åŠ¡ID | int | task_worker (Ã—3) | å¹¶å‘å¤„ç†é˜Ÿåˆ— |
| **update_queue** | å­˜å‚¨éœ€è¦æ›´æ–°çŠ¶æ€çš„ä»»åŠ¡ID | int | update_task_worker | çŠ¶æ€æ›´æ–°é˜Ÿåˆ— |
| **cleanup_queue** | å­˜å‚¨éœ€è¦æ¸…ç†èµ„æºçš„ä»»åŠ¡ID | int | cleanup_worker | èµ„æºæ¸…ç†é˜Ÿåˆ— |
| **callback_queue** | å­˜å‚¨éœ€è¦å›è°ƒçš„ä»»åŠ¡ID | int | callback_worker | å›è°ƒé€šçŸ¥é˜Ÿåˆ— |

### é˜Ÿåˆ—ç›‘æ§

```python
def get_queue_stats(self) -> Dict[str, int]:
    """è·å–é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯"""
    return {
        'fetch_queue': self.fetch_queue.qsize(),
        'processing_queue': self.task_processing_queue.qsize(),
        'update_queue': self.update_queue.qsize(),
        'cleanup_queue': self.cleanup_queue.qsize(),
        'callback_queue': self.callback_queue.qsize(),
        'total_tasks': len(self.tasks),
        'pending_tasks': len([t for t in self.tasks.values() if t.status == 'pending']),
        'processing_tasks': len([t for t in self.tasks.values() if t.status == 'processing']),
        'completed_tasks': len([t for t in self.tasks.values() if t.status == 'completed']),
        'failed_tasks': len([t for t in self.tasks.values() if t.status == 'failed'])
    }

## âš ï¸ é”™è¯¯å¤„ç†æœºåˆ¶

### é‡è¯•ç­–ç•¥

ç³»ç»Ÿé‡‡ç”¨æ™ºèƒ½é‡è¯•æœºåˆ¶ï¼Œç¡®ä¿ä»»åŠ¡çš„å¯é æ‰§è¡Œï¼š

```mermaid
flowchart TD
    A[ä»»åŠ¡æ‰§è¡Œ] --> B{æ‰§è¡ŒæˆåŠŸ?}
    B -->|æ˜¯| C[æ ‡è®°ä¸ºcompleted]
    B -->|å¦| D[å¢åŠ retry_count]
    D --> E{retry_count < max_retries?}
    E -->|æ˜¯| F[é‡æ–°æ”¾å…¥fetch_queue]
    E -->|å¦| G[æ ‡è®°ä¸ºfailed]
    F --> H[ç­‰å¾…é‡æ–°å¤„ç†]
    H --> A
    C --> I[ä»»åŠ¡å®Œæˆ]
    G --> J[ä»»åŠ¡å¤±è´¥]
```

### é”™è¯¯åˆ†ç±»ä¸å¤„ç†

#### MinerUè½¬æ¢é”™è¯¯åˆ†æ

```python
def _analyze_mineru_python_error(self, error_str: str, traceback_str: str) -> str:
    """åˆ†æMinerU Python APIé”™è¯¯ä¿¡æ¯"""
    full_error = error_str + " " + traceback_str

    if "CUDA out of memory" in full_error:
        return "GPUå†…å­˜ä¸è¶³é”™è¯¯ - éœ€è¦é‡Šæ”¾GPUå†…å­˜æˆ–ä½¿ç”¨æ›´å°çš„batch size"
    elif "No module named" in full_error:
        return "Pythonæ¨¡å—ç¼ºå¤±é”™è¯¯ - æ£€æŸ¥MinerUåŠå…¶ä¾èµ–æ˜¯å¦æ­£ç¡®å®‰è£…"
    elif "CUDA" in full_error and "not available" in full_error:
        return "CUDAä¸å¯ç”¨é”™è¯¯ - æ£€æŸ¥CUDAé©±åŠ¨ã€PyTorchå’ŒGPUè®¾ç½®"
    elif "Permission denied" in full_error:
        return "æƒé™é”™è¯¯ - æ£€æŸ¥æ–‡ä»¶å’Œç›®å½•çš„è¯»å†™æƒé™"
    elif "FileNotFoundError" in full_error:
        return "æ–‡ä»¶æœªæ‰¾åˆ°é”™è¯¯ - æ£€æŸ¥è¾“å…¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®"
    else:
        return f"æœªçŸ¥é”™è¯¯ - {error_str[:200]}..."
```

#### å¸¸è§é”™è¯¯ç±»å‹

| é”™è¯¯ç±»å‹ | åŸå›  | è§£å†³æ–¹æ¡ˆ | é‡è¯•ç­–ç•¥ |
|----------|------|----------|----------|
| **GPUå†…å­˜ä¸è¶³** | CUDA OOM | æ¸…ç†GPUå†…å­˜ï¼Œå‡å°‘batch size | è‡ªåŠ¨é‡è¯• |
| **æ–‡ä»¶æƒé™é”™è¯¯** | æƒé™ä¸è¶³ | æ£€æŸ¥æ–‡ä»¶è¯»å†™æƒé™ | ä¸é‡è¯• |
| **æ¨¡å—å¯¼å…¥é”™è¯¯** | ä¾èµ–ç¼ºå¤± | å®‰è£…ç¼ºå¤±çš„PythonåŒ… | ä¸é‡è¯• |
| **LibreOfficeé”™è¯¯** | è½¬æ¢å¤±è´¥ | æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’ŒLibreOfficeçŠ¶æ€ | è‡ªåŠ¨é‡è¯• |
| **ç½‘ç»œé”™è¯¯** | æ¨¡å‹ä¸‹è½½å¤±è´¥ | æ£€æŸ¥ç½‘ç»œè¿æ¥ | è‡ªåŠ¨é‡è¯• |

### GPUå†…å­˜ç®¡ç†

```python
def _clear_gpu_memory(self):
    """æ¸…ç†GPUå†…å­˜"""
    try:
        import torch
        import gc
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()
            self.logger.info("GPU memory cleared")
    except Exception as e:
        self.logger.warning(f"Failed to clear GPU memory: {e}")
```

## ğŸš€ æ€§èƒ½ä¼˜åŒ–

### é…ç½®å‚æ•°ä¼˜åŒ–

#### TaskProcessoré…ç½®

```python
# ç”Ÿäº§ç¯å¢ƒæ¨èé…ç½®
task_processor = TaskProcessor(
    max_concurrent_tasks=3,      # æ ¹æ®GPUå†…å­˜è°ƒæ•´
    task_check_interval=5,       # æ£€æŸ¥é—´éš”(ç§’)
    workspace_dir="/app/task_workspace"
)
```

#### å¹¶å‘æ•°è°ƒä¼˜æŒ‡å—

| èµ„æºç±»å‹ | æ¨èå¹¶å‘æ•° | è¯´æ˜ |
|----------|------------|------|
| **CPUå¯†é›†å‹** | CPUæ ¸å¿ƒæ•° | LibreOfficeè½¬æ¢ |
| **GPUå¯†é›†å‹** | 1-2 | MinerUè½¬æ¢ï¼Œå—GPUå†…å­˜é™åˆ¶ |
| **IOå¯†é›†å‹** | CPUæ ¸å¿ƒæ•° Ã— 2-4 | æ–‡ä»¶è¯»å†™æ“ä½œ |
| **æ··åˆä»»åŠ¡** | 2-3 | å¹³è¡¡CPUå’ŒGPUä½¿ç”¨ |

### å†…å­˜ä¼˜åŒ–

#### 1. GPUå†…å­˜ç®¡ç†

```python
# åœ¨æ¯ä¸ªä»»åŠ¡å®Œæˆåæ¸…ç†GPUå†…å­˜
async def _process_pdf_to_markdown(self, task: Task, workspace: Path):
    try:
        # æ‰§è¡Œè½¬æ¢
        result = await doc_service.convert_pdf_to_markdown(...)
        return result
    finally:
        # ç¡®ä¿æ¸…ç†GPUå†…å­˜
        self._clear_gpu_memory()
```

#### 2. ä¸´æ—¶æ–‡ä»¶æ¸…ç†

```python
async def _cleanup_worker(self):
    """æ¸…ç†å·¥ä½œåç¨‹"""
    while self.is_running:
        try:
            task_id = await asyncio.wait_for(
                self.cleanup_queue.get(),
                timeout=self.task_check_interval
            )

            # æ¸…ç†ä»»åŠ¡å·¥ä½œç›®å½•
            task_workspace = self.workspace_dir / f"task_{task_id}"
            if task_workspace.exists():
                shutil.rmtree(task_workspace)
                self.logger.debug(f"Cleaned up workspace for task {task_id}")

        except asyncio.TimeoutError:
            continue
```

### ç›‘æ§å’Œè°ƒè¯•

#### 1. æ€§èƒ½æŒ‡æ ‡ç›‘æ§

```python
# æ·»åŠ æ€§èƒ½ç›‘æ§
import time
from collections import defaultdict

class PerformanceMonitor:
    def __init__(self):
        self.task_times = defaultdict(list)
        self.error_counts = defaultdict(int)

    def record_task_time(self, task_type: str, duration: float):
        self.task_times[task_type].append(duration)

    def get_average_time(self, task_type: str) -> float:
        times = self.task_times[task_type]
        return sum(times) / len(times) if times else 0
```

#### 2. æ—¥å¿—é…ç½®

```python
# è¯¦ç»†çš„æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('task_processor.log'),
        logging.StreamHandler()
    ]
)
```

## ğŸ³ éƒ¨ç½²æŒ‡å—

### Dockeréƒ¨ç½²

#### 1. Dockerfile

```dockerfile
# åŸºäºMinerUåŸºç¡€é•œåƒ
FROM docker.cnb.cool/aiedulab/library/mineru:latest

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /workspace

# å¤åˆ¶é¡¹ç›®æ–‡ä»¶
COPY api/ /workspace/api/
COPY processors/ /workspace/processors/
COPY services/ /workspace/services/
COPY start.py /workspace/

# å®‰è£…ä¾èµ–
RUN pip install --no-cache-dir \
    fastapi==0.104.1 \
    uvicorn[standard]==0.24.0 \
    aiofiles==23.2.1 \
    python-multipart==0.0.6

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["python", "/workspace/start.py"]
```

#### 2. Docker Compose

```yaml
version: '3.8'
services:
  document-scheduler:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./input:/workspace/input
      - ./output:/workspace/output
    environment:
      - PYTHONPATH=/workspace
      - PYTHONUNBUFFERED=1
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

#### 1. ç³»ç»Ÿè¦æ±‚

| ç»„ä»¶ | æœ€ä½è¦æ±‚ | æ¨èé…ç½® |
|------|----------|----------|
| **CPU** | 4æ ¸å¿ƒ | 8æ ¸å¿ƒ+ |
| **å†…å­˜** | 8GB | 16GB+ |
| **GPU** | GTX 1080 | RTX 3090/4090 |
| **å­˜å‚¨** | 50GB | 200GB+ SSD |
| **ç½‘ç»œ** | 100Mbps | 1Gbps |

#### 2. ç¯å¢ƒé…ç½®

```bash
# 1. å®‰è£…ä¾èµ–
sudo apt-get update
sudo apt-get install -y libreoffice python3-pip

# 2. å®‰è£…Pythonä¾èµ–
pip install -r requirements.txt

# 3. é…ç½®ç¯å¢ƒå˜é‡
export PYTHONPATH=/workspace
export CUDA_VISIBLE_DEVICES=0

# 4. å¯åŠ¨æœåŠ¡
python start.py
```

#### 3. Nginxåå‘ä»£ç†

```nginx
server {
    listen 80;
    server_name your-domain.com;

    location / {
        proxy_pass http://127.0.0.1:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        # å¢åŠ è¶…æ—¶æ—¶é—´ï¼Œé€‚åº”é•¿æ—¶é—´çš„æ–‡æ¡£è½¬æ¢
        proxy_read_timeout 300s;
        proxy_connect_timeout 75s;
    }
}
```

## ğŸ”§ æ‰©å±•å¼€å‘

### æ·»åŠ æ–°çš„è½¬æ¢ç±»å‹

#### 1. åœ¨TaskProcessorä¸­æ³¨å†Œå¤„ç†å™¨

```python
# processors/task_processor.py
self.processors = {
    'office_to_pdf': self._process_office_to_pdf,
    'pdf_to_markdown': self._process_pdf_to_markdown,
    'office_to_markdown': self._process_office_to_markdown,
    'new_conversion_type': self._process_new_conversion,  # æ–°å¢
}
```

#### 2. å®ç°å¤„ç†æ–¹æ³•

```python
async def _process_new_conversion(self, task: Task, workspace: Path) -> Dict[str, Any]:
    """å¤„ç†æ–°çš„è½¬æ¢ç±»å‹"""
    try:
        # å®ç°å…·ä½“çš„è½¬æ¢é€»è¾‘
        from services.custom_service import CustomConverter

        converter = CustomConverter()
        result = await converter.convert(
            input_path=task.input_path,
            output_path=task.output_path,
            params=task.params
        )

        if result['success']:
            return {
                'success': True,
                'input_file': task.input_path,
                'output_file': task.output_path,
                'conversion_type': 'new_conversion_type'
            }
        else:
            raise Exception(result.get('error', 'Conversion failed'))

    except Exception as e:
        self.logger.error(f"New conversion failed: {e}")
        raise
```

#### 3. åœ¨APIä¸­æ·»åŠ æ”¯æŒ

```python
# api/main.py
valid_task_types = {
    'office_to_pdf', 'pdf_to_markdown',
    'office_to_markdown', 'batch_office_to_markdown',
    'new_conversion_type'  # æ–°å¢
}
```

### è‡ªå®šä¹‰é”™è¯¯å¤„ç†

```python
class CustomErrorHandler:
    def analyze_error(self, error: Exception, task: Task) -> str:
        """è‡ªå®šä¹‰é”™è¯¯åˆ†æé€»è¾‘"""
        if isinstance(error, CustomConversionError):
            return "è‡ªå®šä¹‰è½¬æ¢é”™è¯¯ - æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ ¼å¼"
        elif isinstance(error, NetworkError):
            return "ç½‘ç»œé”™è¯¯ - æ£€æŸ¥ç½‘ç»œè¿æ¥"
        return "æœªçŸ¥é”™è¯¯"

    def should_retry(self, error: Exception, retry_count: int) -> bool:
        """è‡ªå®šä¹‰é‡è¯•ç­–ç•¥"""
        if isinstance(error, TemporaryError) and retry_count < 5:
            return True
        if isinstance(error, NetworkError) and retry_count < 3:
            return True
        return False
```

### æ·»åŠ æ–°çš„é˜Ÿåˆ—

```python
# æ·»åŠ ä¼˜å…ˆçº§é˜Ÿåˆ—
class PriorityTaskProcessor(TaskProcessor):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.high_priority_queue = asyncio.Queue()
        self.normal_priority_queue = asyncio.Queue()
        self.low_priority_queue = asyncio.Queue()

    async def create_task(self, priority='normal', **kwargs):
        task_id = await super().create_task(priority=priority, **kwargs)

        # æ ¹æ®ä¼˜å…ˆçº§æ”¾å…¥ä¸åŒé˜Ÿåˆ—
        if priority == 'high':
            await self.high_priority_queue.put(task_id)
        elif priority == 'low':
            await self.low_priority_queue.put(task_id)
        else:
            await self.normal_priority_queue.put(task_id)

        return task_id
```

## ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•

### æµ‹è¯•ç¯å¢ƒ

| é…ç½®é¡¹ | è§„æ ¼ |
|--------|------|
| **CPU** | Intel i7-10700K (8æ ¸16çº¿ç¨‹) |
| **GPU** | NVIDIA RTX 3090 (24GB VRAM) |
| **å†…å­˜** | 32GB DDR4 |
| **å­˜å‚¨** | 1TB NVMe SSD |

### æ€§èƒ½æ•°æ®

| è½¬æ¢ç±»å‹ | æ–‡ä»¶å¤§å° | å¤„ç†æ—¶é—´ | æˆåŠŸç‡ | å¹¶å‘æ•° |
|----------|----------|----------|--------|--------|
| **office_to_pdf** | 1-5MB | 5-15ç§’ | 99% | 3 |
| **pdf_to_markdown** | 1-10MB | 30-120ç§’ | 95% | 1 |
| **office_to_markdown** | 1-5MB | 35-135ç§’ | 94% | 1 |
| **batch_processing** | 10-50æ–‡ä»¶ | 5-30åˆ†é’Ÿ | 96% | 3 |

### ä¼˜åŒ–å»ºè®®

1. **GPUå†…å­˜ä¼˜åŒ–**: å®šæœŸæ¸…ç†GPUå†…å­˜ï¼Œé¿å…OOM
2. **å¹¶å‘æ§åˆ¶**: æ ¹æ®ç¡¬ä»¶é…ç½®è°ƒæ•´å¹¶å‘æ•°
3. **æ–‡ä»¶é¢„å¤„ç†**: æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œå¤§å°
4. **ç›‘æ§å‘Šè­¦**: è®¾ç½®é˜Ÿåˆ—é•¿åº¦å’Œå¤„ç†æ—¶é—´å‘Šè­¦
5. **è´Ÿè½½å‡è¡¡**: å¤šå®ä¾‹éƒ¨ç½²ï¼Œåˆ†æ•£å¤„ç†å‹åŠ›

## ğŸ“ æ€»ç»“

æ–‡æ¡£è½¬æ¢è°ƒåº¦ç³»ç»Ÿé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å¼‚æ­¥ä»»åŠ¡å¤„ç†æ¶æ„ï¼Œå®ç°äº†é«˜æ•ˆã€å¯é çš„æ–‡æ¡£è½¬æ¢æœåŠ¡ã€‚ç³»ç»Ÿçš„æ ¸å¿ƒä¼˜åŠ¿åŒ…æ‹¬ï¼š

### æŠ€æœ¯äº®ç‚¹

- **å¼‚æ­¥å¹¶å‘**: å¤šé˜Ÿåˆ— + å¤šåç¨‹è®¾è®¡ï¼Œæ”¯æŒé«˜å¹¶å‘å¤„ç†
- **æ™ºèƒ½é‡è¯•**: è‡ªåŠ¨é”™è¯¯åˆ†æå’Œé‡è¯•æœºåˆ¶
- **å®æ—¶ç›‘æ§**: å®Œæ•´çš„ä»»åŠ¡çŠ¶æ€è·Ÿè¸ªå’Œç»Ÿè®¡
- **æ¨¡å—åŒ–**: æ¸…æ™°çš„åˆ†å±‚æ¶æ„ï¼Œæ˜“äºæ‰©å±•å’Œç»´æŠ¤
- **é«˜å¯ç”¨**: å®Œå–„çš„é”™è¯¯å¤„ç†å’Œèµ„æºç®¡ç†

### é€‚ç”¨åœºæ™¯

- **ä¼ä¸šæ–‡æ¡£ç®¡ç†**: å¤§é‡Officeæ–‡æ¡£çš„æ‰¹é‡è½¬æ¢
- **å†…å®¹å¤„ç†å¹³å°**: PDFæ–‡æ¡£çš„ç»“æ„åŒ–æå–
- **çŸ¥è¯†ç®¡ç†ç³»ç»Ÿ**: æ–‡æ¡£æ ¼å¼æ ‡å‡†åŒ–
- **è‡ªåŠ¨åŒ–å·¥ä½œæµ**: æ–‡æ¡£å¤„ç†æµæ°´çº¿

è¿™ä¸ªç³»ç»Ÿä¸ºæ–‡æ¡£å¤„ç†è‡ªåŠ¨åŒ–æä¾›äº†ä¸€ä¸ªç”Ÿäº§çº§çš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰è‰¯å¥½çš„æ€§èƒ½ã€å¯é æ€§å’Œæ‰©å±•æ€§ã€‚
```
