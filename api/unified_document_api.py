#!/usr/bin/env python3
"""
ç»Ÿä¸€æ–‡æ¡£è½¬æ¢API
å¤åˆ»MediaConvertçš„ç»Ÿä¸€ä»»åŠ¡åˆ›å»ºæ¥å£ï¼Œæ”¯æŒå¤šç§è¾“å…¥æ–¹å¼å’Œä»»åŠ¡ç±»å‹
"""

from typing import Optional, Union
from fastapi import APIRouter, Request, Form, File, UploadFile, HTTPException, Depends
from fastapi.responses import JSONResponse

from database.models import TaskCreateRequest, TaskResponse, DocumentTask, QueryTasksFilter, TaskStatistics
from processors.enhanced_task_processor import EnhancedTaskProcessor
from utils.logging_utils import configure_logging

router = APIRouter()
logger = configure_logging(name=__name__)

# å…¨å±€ä»»åŠ¡å¤„ç†å™¨å®ä¾‹
task_processor: Optional[EnhancedTaskProcessor] = None


async def get_task_processor() -> EnhancedTaskProcessor:
    """è·å–ä»»åŠ¡å¤„ç†å™¨å®ä¾‹"""
    global task_processor
    if task_processor is None:
        raise HTTPException(status_code=503, detail="Task processor not initialized")
    return task_processor


def initialize_task_processor(database_type: str = "sqlite", 
                            database_url: str = "sqlite:///./document_tasks.db"):
    """åˆå§‹åŒ–ä»»åŠ¡å¤„ç†å™¨"""
    global task_processor
    task_processor = EnhancedTaskProcessor(
        database_type=database_type,
        database_url=database_url
    )


@router.get("/health", summary="å¥åº·æ£€æŸ¥", description="æ£€æŸ¥æ–‡æ¡£è½¬æ¢APIæœåŠ¡çŠ¶æ€")
async def health_check():
    """
    å¥åº·æ£€æŸ¥ç«¯ç‚¹
    
    Returns:
        dict: æœåŠ¡çŠ¶æ€ä¿¡æ¯
    """
    try:
        processor = await get_task_processor()
        stats = processor.get_stats()
        
        return {
            "status": "healthy",
            "message": "Document Conversion API is running",
            "processor_running": stats["is_running"],
            "total_tasks": stats["total_tasks"],
            "active_tasks": stats["active_tasks"]
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "message": f"Service error: {str(e)}"
        }


@router.post(
    "/tasks/create",
    response_model=TaskResponse,
    summary="åˆ›å»ºæ–‡æ¡£è½¬æ¢ä»»åŠ¡",
    description="""
    ## åˆ›å»ºæ–‡æ¡£è½¬æ¢ä»»åŠ¡

    æ”¯æŒä»S3å­˜å‚¨åˆ›å»ºå„ç§ç±»å‹çš„æ–‡æ¡£è½¬æ¢ä»»åŠ¡ã€‚

    ### ğŸ“‹ æ”¯æŒçš„ä»»åŠ¡ç±»å‹
    - **pdf_to_markdown**: PDFè½¬Markdownï¼Œè¾“å‡º.mdæ–‡ä»¶ã€.jsonç»“æ„æ–‡ä»¶å’Œå›¾ç‰‡
    - **office_to_pdf**: Officeæ–‡æ¡£è½¬PDFï¼Œæ”¯æŒ.doc/.docx/.xls/.xlsx/.ppt/.pptx
    - **office_to_markdown**: Officeæ–‡æ¡£è½¬Markdownï¼Œä¸¤æ­¥è½¬æ¢(å…ˆè½¬PDFå†è½¬Markdown)

    ### ğŸ“ S3è·¯å¾„è§„åˆ™
    **è¾“å…¥è·¯å¾„æ ¼å¼**: `s3://{bucket_name}/{file_path}`

    **è¾“å‡ºè·¯å¾„æ ¼å¼**: `s3://ai-file/{original_bucket}/{file_name_without_ext}/{conversion_type}/`

    ### ğŸ“Š ä¼˜å…ˆçº§è¯´æ˜
    - **high**: é«˜ä¼˜å…ˆçº§ï¼Œç«‹å³å¤„ç†
    - **normal**: æ™®é€šä¼˜å…ˆçº§ï¼ŒæŒ‰é˜Ÿåˆ—é¡ºåºå¤„ç†
    - **low**: ä½ä¼˜å…ˆçº§ï¼Œåœ¨å…¶ä»–ä»»åŠ¡å®Œæˆåå¤„ç†

    ### ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹
    ```bash
    # PDFè½¬Markdown
    curl -X POST "http://localhost:8000/api/tasks/create" \\
      -F "task_type=pdf_to_markdown" \\
      -F "bucket_name=documents" \\
      -F "file_path=reports/annual_report.pdf" \\
      -F "platform=your-platform" \\
      -F "priority=high"

    # Officeè½¬PDF
    curl -X POST "http://localhost:8000/api/tasks/create" \\
      -F "task_type=office_to_pdf" \\
      -F "bucket_name=documents" \\
      -F "file_path=presentations/quarterly.pptx" \\
      -F "platform=your-platform" \\
      -F "priority=normal"
    ```
    """,
    responses={
        200: {
            "description": "ä»»åŠ¡åˆ›å»ºæˆåŠŸ",
            "content": {
                "application/json": {
                    "example": {
                        "task_id": 123,
                        "message": "Document conversion task 123 created successfully",
                        "status": "pending"
                    }
                }
            }
        },
        400: {
            "description": "è¯·æ±‚å‚æ•°é”™è¯¯",
            "content": {
                "application/json": {
                    "example": {
                        "detail": "Invalid task_type. Supported types: pdf_to_markdown, office_to_pdf, office_to_markdown"
                    }
                }
            }
        },
        500: {
            "description": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯",
            "content": {
                "application/json": {
                    "example": {
                        "detail": "Internal server error: Failed to create task"
                    }
                }
            }
        }
    }
)
async def create_document_task(
    request: Request,
    
    # === æ–‡ä»¶è¾“å…¥æ–¹å¼ï¼ˆä¸‰é€‰ä¸€ï¼‰ ===
    bucket_name: Optional[str] = Form(None, description="S3å­˜å‚¨æ¡¶åç§°"),
    file_path: Optional[str] = Form(None, description="æ–‡ä»¶åœ¨bucketä¸­çš„è·¯å¾„"),
    file_upload: Optional[UploadFile] = File(None, description="æœ¬åœ°æ–‡ä»¶ä¸Šä¼ "),
    file_url: Optional[str] = Form(None, description="æ–‡ä»¶HTTP URL"),
    input_path: Optional[str] = Form(None, description="æœ¬åœ°æ–‡ä»¶è·¯å¾„"),
    
    # === ä»»åŠ¡åŸºæœ¬å‚æ•° ===
    task_type: str = Form(..., description="ä»»åŠ¡ç±»å‹ï¼šoffice_to_pdf, pdf_to_markdown, office_to_markdown"),
    priority: str = Form("normal", description="ä»»åŠ¡ä¼˜å…ˆçº§ï¼šlow, normal, high"),
    callback_url: Optional[str] = Form(None, description="ä»»åŠ¡å®Œæˆå›è°ƒURL"),
    platform: Optional[str] = Form("gaojiaqi", description="å¹³å°æ ‡è¯†ï¼Œç”¨äºåˆ†ç±»ç®¡ç†"),
    
    # === è¾“å‡ºé…ç½® ===
    output_path: Optional[str] = Form(None, description="è¾“å‡ºæ–‡ä»¶è·¯å¾„"),
    
    # === è½¬æ¢å‚æ•° ===
    params: Optional[str] = Form(None, description="è½¬æ¢å‚æ•°JSONå­—ç¬¦ä¸²"),
    
    processor: EnhancedTaskProcessor = Depends(get_task_processor)
) -> TaskResponse:
    """
    # ç»Ÿä¸€æ–‡æ¡£è½¬æ¢ä»»åŠ¡åˆ›å»ºæ¥å£
    
    ## åŠŸèƒ½è¯´æ˜
    
    è¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ–‡æ¡£è½¬æ¢ä»»åŠ¡åˆ›å»ºæ¥å£ï¼Œæ”¯æŒå¤šç§ä»»åŠ¡ç±»å‹å’Œè¾“å…¥æ–¹å¼ï¼Œå¤åˆ»MediaConvertçš„è®¾è®¡ã€‚
    
    ## æ”¯æŒçš„ä»»åŠ¡ç±»å‹

    - **office_to_pdf**: Officeæ–‡æ¡£è½¬PDF
    - **pdf_to_markdown**: PDFè½¬Markdown
    - **office_to_markdown**: Officeæ–‡æ¡£ç›´æ¥è½¬Markdown
    - **image_to_markdown**: å›¾ç‰‡è½¬Markdownï¼ˆOCRè¯†åˆ«ï¼‰
    - **batch_office_to_pdf**: æ‰¹é‡Officeè½¬PDF
    - **batch_pdf_to_markdown**: æ‰¹é‡PDFè½¬Markdown
    - **batch_office_to_markdown**: æ‰¹é‡Officeè½¬Markdown
    - **batch_image_to_markdown**: æ‰¹é‡å›¾ç‰‡è½¬Markdown

    ## ä»»åŠ¡é‡è¯•åŠŸèƒ½

    ### å•ä¸ªä»»åŠ¡é‡è¯•
    ```bash
    curl -X POST "http://localhost:8000/api/tasks/{task_id}/retry"
    ```

    ### æ‰¹é‡é‡è¯•å¤±è´¥ä»»åŠ¡
    ```bash
    curl -X POST "http://localhost:8000/api/tasks/retry-failed"
    ```

    é‡è¯•åŠŸèƒ½ä¼šï¼š
    - é‡ç½®ä»»åŠ¡çŠ¶æ€ä¸ºpending
    - æ¸…é™¤é”™è¯¯ä¿¡æ¯
    - é‡æ–°æ”¾å…¥å¤„ç†é˜Ÿåˆ—
    - é‡ç½®é‡è¯•è®¡æ•°å™¨
    
    ## è¾“å…¥æ–¹å¼
    
    æ”¯æŒä¸‰ç§è¾“å…¥æ–¹å¼ï¼ˆä¸‰é€‰ä¸€ï¼‰ï¼š
    
    1. **S3å­˜å‚¨**: æä¾› `bucket_name` å’Œ `file_path`
    2. **HTTP URL**: æä¾› `file_url`
    3. **æœ¬åœ°æ–‡ä»¶**: æä¾› `input_path` æˆ–ä¸Šä¼  `file_upload`
    
    ## è¾“å‡ºé…ç½®
    
    - è½¬æ¢ç»“æœè‡ªåŠ¨ä¸Šä¼ åˆ° `ai-file` å­˜å‚¨æ¡¶
    - æ”¯æŒè‡ªå®šä¹‰è¾“å‡ºè·¯å¾„
    - è‡ªåŠ¨ç”Ÿæˆè®¿é—®URL
    
    ## ç¤ºä¾‹
    
    ### S3è¾“å…¥ç¤ºä¾‹
    ```bash
    curl -X POST "http://localhost:8000/api/tasks/create" \
         -F "task_type=pdf_to_markdown" \
         -F "bucket_name=ai-file" \
         -F "file_path=2024æœ¬ç§‘ç”Ÿå­¦ç”Ÿæ‰‹å†Œ.pdf" \
         -F "platform=gaojiaqi" \
         -F "priority=normal"
    ```
    
    ### æœ¬åœ°æ–‡ä»¶ä¸Šä¼ ç¤ºä¾‹
    ```bash
    curl -X POST "http://localhost:8000/api/tasks/create" \
         -F "task_type=office_to_pdf" \
         -F "file_upload=@document.docx" \
         -F "platform=gaojiaqi"
    ```
    """
    try:
        # éªŒè¯è¾“å…¥æ–¹å¼
        input_count = sum([
            bool(bucket_name and file_path),
            bool(file_url),
            bool(input_path),
            bool(file_upload)
        ])
        
        if input_count == 0:
            raise HTTPException(
                status_code=400,
                detail="Must provide one input method: (bucket_name + file_path), file_url, input_path, or file_upload"
            )
        
        if input_count > 1:
            raise HTTPException(
                status_code=400,
                detail="Only one input method allowed"
            )
        
        # å¤„ç†æ–‡ä»¶ä¸Šä¼ 
        if file_upload:
            # TODO: å®ç°æ–‡ä»¶ä¸Šä¼ é€»è¾‘
            raise HTTPException(status_code=501, detail="File upload not implemented yet")
        
        # è§£æå‚æ•°
        task_params = {}
        if params:
            import json
            try:
                task_params = json.loads(params)
            except json.JSONDecodeError:
                raise HTTPException(status_code=400, detail="Invalid params JSON")
        
        # éªŒè¯ä¼˜å…ˆçº§
        if priority not in ["low", "normal", "high"]:
            raise HTTPException(status_code=400, detail="Invalid priority. Must be: low, normal, high")
        
        # åº”ç”¨MediaConvertçš„ä¸­æ–‡æ–‡ä»¶åå¤„ç†æ–¹æ¡ˆ
        if file_path:
            from utils.encoding_utils import EncodingUtils
            original_file_path = file_path
            file_path = EncodingUtils.fix_file_path_encoding(file_path)
            if file_path != original_file_path:
                logger.info(f"Fixed file_path encoding: {original_file_path} -> {file_path}")

        # åˆ›å»ºä»»åŠ¡è¯·æ±‚
        task_request = TaskCreateRequest(
            task_type=task_type,
            priority=priority,
            bucket_name=bucket_name,
            file_path=file_path,
            file_url=file_url,
            input_path=input_path,
            output_path=output_path,
            params=task_params,
            callback_url=callback_url,
            platform=platform
        )
        
        # åˆ›å»ºä»»åŠ¡
        task_id = await processor.create_task(task_request)
        
        logger.info(f"Created document conversion task {task_id} for platform {platform}")
        
        return TaskResponse(
            task_id=task_id,
            message=f"Document conversion task {task_id} created successfully",
            status="pending"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to create document task: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")


@router.put("/tasks/{task_id}/task-type", summary="ä¿®æ”¹ä»»åŠ¡ç±»å‹")
async def update_task_type(
    task_id: int,
    new_task_type: str = Form(..., description="æ–°çš„ä»»åŠ¡ç±»å‹"),
    processor: EnhancedTaskProcessor = Depends(get_task_processor)
):
    """ä¿®æ”¹ä»»åŠ¡çš„ç±»å‹ï¼Œé€‚ç”¨äºç±»å‹ä¸åŒ¹é…çš„å¤±è´¥ä»»åŠ¡"""
    try:
        # éªŒè¯ä»»åŠ¡ç±»å‹
        valid_types = [
            "office_to_pdf", "pdf_to_markdown", "office_to_markdown", "image_to_markdown",
            "batch_office_to_pdf", "batch_pdf_to_markdown", "batch_office_to_markdown", "batch_image_to_markdown"
        ]

        if new_task_type not in valid_types:
            raise HTTPException(
                status_code=400,
                detail=f"Invalid task type. Supported types: {', '.join(valid_types)}"
            )

        # è·å–ä»»åŠ¡
        task = await processor.db_manager.get_task(str(task_id))
        if not task:
            raise HTTPException(status_code=404, detail="Task not found")

        # åªå…è®¸ä¿®æ”¹å¤±è´¥çš„ä»»åŠ¡
        if task.status not in ["failed"]:
            raise HTTPException(
                status_code=400,
                detail=f"Can only modify failed tasks. Current status: {task.status}"
            )

        # æ›´æ–°ä»»åŠ¡ç±»å‹
        success = await processor.db_manager.update_task(
            str(task_id),
            task_type=new_task_type
        )

        if not success:
            raise HTTPException(status_code=500, detail="Failed to update task type")

        logger.info(f"Task {task_id} type updated from {task.task_type} to {new_task_type}")

        return {
            "message": f"Task {task_id} type updated successfully",
            "task_id": task_id,
            "old_task_type": task.task_type,
            "new_task_type": new_task_type,
            "status": "ready_for_retry"
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to update task type for task {task_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")


@router.get(
    "/tasks/{task_id}",
    summary="è·å–ä»»åŠ¡è¯¦æƒ…",
    description="""
    ## è·å–ä»»åŠ¡è¯¦ç»†ä¿¡æ¯

    æ ¹æ®ä»»åŠ¡IDè·å–ä»»åŠ¡çš„å®Œæ•´çŠ¶æ€ä¿¡æ¯ï¼ŒåŒ…æ‹¬è¾“å…¥è¾“å‡ºè·¯å¾„ã€å¤„ç†æ—¶é—´ã€S3 URLsç­‰ã€‚

    ### ğŸ“Š ä»»åŠ¡çŠ¶æ€è¯´æ˜
    - **pending**: ç­‰å¾…å¤„ç†
    - **processing**: æ­£åœ¨å¤„ç†
    - **completed**: å¤„ç†å®Œæˆ
    - **failed**: å¤„ç†å¤±è´¥

    ### ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹
    ```bash
    curl "http://localhost:8000/api/tasks/123"
    ```

    ### ğŸ“ å“åº”ä¸­çš„é‡è¦å­—æ®µ
    - **output_url**: ä¸»è¦è¾“å‡ºæ–‡ä»¶çš„S3è·¯å¾„
    - **s3_urls**: æ‰€æœ‰è¾“å‡ºæ–‡ä»¶çš„S3è·¯å¾„åˆ—è¡¨
    - **task_processing_time**: ä»»åŠ¡å¤„ç†è€—æ—¶(ç§’)
    - **file_size_bytes**: è¾“å…¥æ–‡ä»¶å¤§å°
    """,
    responses={
        200: {
            "description": "ä»»åŠ¡è¯¦æƒ…",
            "content": {
                "application/json": {
                    "example": {
                        "id": 123,
                        "task_type": "pdf_to_markdown",
                        "status": "completed",
                        "priority": "high",
                        "bucket_name": "documents",
                        "file_path": "reports/annual_report.pdf",
                        "platform": "your-platform",
                        "input_path": "/app/task_workspace/task_123/input/annual_report.pdf",
                        "output_path": "/app/task_workspace/task_123/output/annual_report.md",
                        "output_url": "s3://ai-file/documents/annual_report/markdown/annual_report.md",
                        "s3_urls": [
                            "s3://ai-file/documents/annual_report/markdown/annual_report.md",
                            "s3://ai-file/documents/annual_report/markdown/annual_report.json",
                            "s3://ai-file/documents/annual_report/markdown/images/chart1.jpg",
                            "s3://ai-file/documents/annual_report/markdown/images/table1.jpg"
                        ],
                        "file_size_bytes": 1048576,
                        "created_at": "2025-08-09T10:00:00",
                        "started_at": "2025-08-09T10:01:00",
                        "completed_at": "2025-08-09T10:03:30",
                        "task_processing_time": 150.5,
                        "result": {
                            "success": True,
                            "conversion_type": "pdf_to_markdown",
                            "upload_result": {
                                "success": True,
                                "total_files": 4,
                                "total_size": 2097152
                            }
                        },
                        "error_message": None
                    }
                }
            }
        },
        404: {
            "description": "ä»»åŠ¡ä¸å­˜åœ¨",
            "content": {
                "application/json": {
                    "example": {
                        "detail": "Task 123 not found"
                    }
                }
            }
        }
    }
)
async def get_task(
    task_id: str,
    processor: EnhancedTaskProcessor = Depends(get_task_processor)
):
    """è·å–æŒ‡å®šä»»åŠ¡çš„è¯¦ç»†ä¿¡æ¯"""
    try:
        task = await processor.db_manager.get_task(task_id)
        if not task:
            raise HTTPException(status_code=404, detail="Task not found")
        
        return task.to_dict()
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get task {task_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")


@router.get(
    "/tasks",
    summary="æŸ¥è¯¢ä»»åŠ¡åˆ—è¡¨",
    description="""
    ## æŸ¥è¯¢ä»»åŠ¡åˆ—è¡¨

    æ”¯æŒå¤šç§è¿‡æ»¤æ¡ä»¶çš„ä»»åŠ¡åˆ—è¡¨æŸ¥è¯¢ï¼Œå¯ç”¨äºç›‘æ§å’Œç®¡ç†ä»»åŠ¡ã€‚

    ### ğŸ” è¿‡æ»¤å‚æ•°
    - **status**: æŒ‰çŠ¶æ€è¿‡æ»¤ (pending, processing, completed, failed)
    - **priority**: æŒ‰ä¼˜å…ˆçº§è¿‡æ»¤ (high, normal, low)
    - **task_type**: æŒ‰ä»»åŠ¡ç±»å‹è¿‡æ»¤ (pdf_to_markdown, office_to_pdf, office_to_markdown)
    - **platform**: æŒ‰å¹³å°è¿‡æ»¤
    - **limit**: è¿”å›ç»“æœæ•°é‡é™åˆ¶ (é»˜è®¤20)
    - **offset**: åˆ†é¡µåç§»é‡ (é»˜è®¤0)

    ### ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹
    ```bash
    # æŸ¥è¯¢æ‰€æœ‰ä»»åŠ¡
    curl "http://localhost:8000/api/tasks"

    # æŸ¥è¯¢å·²å®Œæˆçš„PDFè½¬Markdownä»»åŠ¡
    curl "http://localhost:8000/api/tasks?status=completed&task_type=pdf_to_markdown&limit=10"

    # åˆ†é¡µæŸ¥è¯¢
    curl "http://localhost:8000/api/tasks?offset=20&limit=10"
    ```
    """,
    responses={
        200: {
            "description": "ä»»åŠ¡åˆ—è¡¨",
            "content": {
                "application/json": {
                    "example": {
                        "tasks": [
                            {
                                "id": 123,
                                "task_type": "pdf_to_markdown",
                                "status": "completed",
                                "priority": "high",
                                "bucket_name": "documents",
                                "file_path": "reports/annual_report.pdf",
                                "platform": "your-platform",
                                "output_url": "s3://ai-file/documents/annual_report/markdown/annual_report.md",
                                "created_at": "2025-08-09T10:00:00",
                                "completed_at": "2025-08-09T10:03:30",
                                "task_processing_time": 150.5
                            }
                        ],
                        "total": 1,
                        "offset": 0,
                        "limit": 20,
                        "filters": {
                            "status": "completed",
                            "task_type": "pdf_to_markdown"
                        }
                    }
                }
            }
        }
    }
)
async def query_tasks(
    status: Optional[str] = None,
    priority: Optional[str] = None,
    task_type: Optional[str] = None,
    platform: Optional[str] = None,
    limit: int = 20,
    offset: int = 0,
    processor: EnhancedTaskProcessor = Depends(get_task_processor)
):
    """æŸ¥è¯¢ä»»åŠ¡åˆ—è¡¨ï¼Œæ”¯æŒå¤šç§è¿‡æ»¤æ¡ä»¶"""
    try:
        filter_params = QueryTasksFilter(
            status=status,
            priority=priority,
            task_type=task_type,
            platform=platform,
            limit=limit,
            offset=offset
        )
        
        tasks = await processor.db_manager.query_tasks(filter_params)
        
        return {
            "tasks": [task.to_dict() for task in tasks],
            "total": len(tasks),
            "limit": limit,
            "offset": offset
        }
        
    except Exception as e:
        logger.error(f"Failed to query tasks: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")


@router.get("/statistics", summary="è·å–ä»»åŠ¡ç»Ÿè®¡")
async def get_statistics(
    processor: EnhancedTaskProcessor = Depends(get_task_processor)
):
    """è·å–ä»»åŠ¡ç»Ÿè®¡ä¿¡æ¯"""
    try:
        # è·å–æ•°æ®åº“ç»Ÿè®¡
        db_stats = await processor.db_manager.get_task_statistics()
        
        # è·å–å¤„ç†å™¨ç»Ÿè®¡
        processor_stats = processor.get_stats()
        
        return {
            "database_statistics": db_stats.dict(),
            "processor_statistics": processor_stats
        }
        
    except Exception as e:
        logger.error(f"Failed to get statistics: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")


@router.post("/tasks/{task_id}/retry", summary="é‡è¯•ä»»åŠ¡")
async def retry_task(
    task_id: str,
    processor: EnhancedTaskProcessor = Depends(get_task_processor)
):
    """æ‰‹åŠ¨é‡è¯•å¤±è´¥çš„ä»»åŠ¡"""
    try:
        task = await processor.db_manager.get_task(task_id)
        if not task:
            raise HTTPException(status_code=404, detail="Task not found")
        
        if task.status not in ["failed", "cancelled"]:
            raise HTTPException(status_code=400, detail="Only failed or cancelled tasks can be retried")
        
        # é‡ç½®ä»»åŠ¡çŠ¶æ€
        await processor.db_manager.update_task(
            task_id,
            status="pending",
            retry_count=0,
            error_message=None
        )
        
        # é‡æ–°æ”¾å…¥é˜Ÿåˆ—
        await processor.fetch_queue.put(task_id)
        
        logger.info(f"Task {task_id} queued for retry")
        
        return {"message": f"Task {task_id} queued for retry"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to retry task {task_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")


@router.post("/tasks/retry-failed", summary="æ‰¹é‡é‡è¯•å¤±è´¥çš„ä»»åŠ¡")
async def retry_failed_tasks(
    processor: EnhancedTaskProcessor = Depends(get_task_processor)
):
    """æ‰¹é‡é‡è¯•æ‰€æœ‰å¤±è´¥çš„ä»»åŠ¡"""
    try:
        # è·å–æ‰€æœ‰å¤±è´¥çš„ä»»åŠ¡
        failed_tasks = await processor.db_manager.get_tasks_by_status("failed")

        if not failed_tasks:
            return {"message": "No failed tasks found", "retried_count": 0}

        retried_tasks = []

        for task in failed_tasks:
            try:
                # é‡ç½®ä»»åŠ¡çŠ¶æ€
                await processor.db_manager.update_task(
                    task.id,
                    status="pending",
                    retry_count=0,
                    error_message=None
                )

                # é‡æ–°æ”¾å…¥é˜Ÿåˆ—
                await processor.fetch_queue.put(task.id)
                retried_tasks.append(task.id)

                logger.info(f"Task {task.id} queued for retry")

            except Exception as e:
                logger.error(f"Failed to retry task {task.id}: {e}")
                continue

        return {
            "message": f"Successfully queued {len(retried_tasks)} failed tasks for retry",
            "retried_count": len(retried_tasks),
            "retried_task_ids": retried_tasks,
            "total_failed_tasks": len(failed_tasks)
        }

    except Exception as e:
        logger.error(f"Failed to retry failed tasks: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")


@router.get("/download/{task_id}/{file_name:path}")
async def download_file(
    task_id: str, 
    file_name: str,
    processor: EnhancedTaskProcessor = Depends(get_task_processor)
):
    """
    ä¸‹è½½è½¬æ¢åçš„æ–‡ä»¶ï¼ˆä»£ç†MinIOä¸‹è½½ï¼‰
    
    Args:
        task_id: ä»»åŠ¡ID
        file_name: æ–‡ä»¶å
    
    Returns:
        æ–‡ä»¶ä¸‹è½½å“åº”
    """
    try:
        # URLè§£ç æ–‡ä»¶å
        from urllib.parse import unquote
        decoded_file_name = unquote(file_name)
        
        # è·å–ä»»åŠ¡ä¿¡æ¯
        task = await processor.db_manager.get_task(task_id)
        if not task:
            raise HTTPException(status_code=404, detail="Task not found")
        
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦æœ‰å¯ä¸‹è½½çš„æ–‡ä»¶ï¼ˆä¸é™åˆ¶å¿…é¡»æ˜¯completedçŠ¶æ€ï¼‰
        if not task.s3_urls:
            raise HTTPException(status_code=404, detail="No files available for download")
        
        # ä»s3_urlsä¸­æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
        s3_urls = task.s3_urls or []
        target_file = None
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼ˆé¿å…ä¸­æ–‡å­—ç¬¦å¯¼è‡´ç¼–ç é—®é¢˜ï¼‰
        logger.info(f"Original file_name length: {len(file_name)}")
        logger.info(f"Decoded file_name length: {len(decoded_file_name)}")
        logger.info(f"Available s3_urls count: {len(s3_urls)}")
        
        for i, s3_url in enumerate(s3_urls):
            logger.info(f"Checking s3_url {i}: length {len(s3_url)}")
            # å°è¯•åŒ¹é…åŸå§‹æ–‡ä»¶åå’Œè§£ç åçš„æ–‡ä»¶å
            if file_name in s3_url or decoded_file_name in s3_url:
                target_file = s3_url
                logger.info(f"Found matching file at index {i}")
                break
        
        if not target_file:
            logger.error(f"File not found. Requested file length: {len(file_name)}, decoded length: {len(decoded_file_name)}, available URLs count: {len(s3_urls)}")
            raise HTTPException(status_code=404, detail="File not found")
        
        # è§£æS3 URLè·å–bucketå’Œkey
        # å‡è®¾æ ¼å¼ä¸º: s3://bucket/path/to/file
        if target_file.startswith('s3://'):
            parts = target_file[5:].split('/', 1)
            if len(parts) != 2:
                raise HTTPException(status_code=400, detail="Invalid S3 URL format")
            
            bucket_name = parts[0]
            s3_key = parts[1]
        else:
            raise HTTPException(status_code=400, detail="Unsupported file URL format")
        
        # ä½¿ç”¨S3DownloadServiceä¸‹è½½æ–‡ä»¶æ•°æ®
        from services.s3_download_service import S3DownloadService
        s3_service = S3DownloadService()
        
        download_result = await s3_service.download_file_data(
            bucket_name=bucket_name,
            s3_key=s3_key
        )
        
        if not download_result.get('success'):
            raise HTTPException(status_code=500, detail=f"Download failed: {download_result.get('error')}")
        
        # è·å–æ–‡ä»¶æ•°æ®å’Œå†…å®¹ç±»å‹
        file_data = download_result['data']
        content_type = download_result.get('content_type', 'application/octet-stream')
        
        # è¿”å›æ–‡ä»¶å“åº”
        from fastapi.responses import StreamingResponse
        from urllib.parse import quote
        import os
        import re
        import io
        
        # è·å–æ–‡ä»¶åï¼ˆä¸åŒ…å«è·¯å¾„ï¼‰
        safe_filename = os.path.basename(decoded_file_name)
        
        # åˆ›å»ºASCIIå®‰å…¨çš„æ–‡ä»¶åç”¨äºContent-Disposition
        # ç§»é™¤æˆ–æ›¿æ¢éASCIIå­—ç¬¦
        ascii_filename = re.sub(r'[^\x00-\x7F]+', '_', safe_filename)
        
        # åˆ›å»ºå­—èŠ‚æµ
        file_stream = io.BytesIO(file_data)
        
        return StreamingResponse(
            io.BytesIO(file_data),
            media_type=content_type,
            headers={
                "Content-Disposition": f"attachment; filename={ascii_filename}",
                "Content-Length": str(len(file_data))
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error downloading file: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")
